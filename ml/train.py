# ml/train.py (heavy MLP version)
import os, time, json
import numpy as np
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

os.environ.setdefault("GIT_PYTHON_REFRESH", "quiet")

import mlflow as mlmod
from mlflow.tracking import MlflowClient
from mlflow.exceptions import RestException
from mlflow.models import infer_signature

from sklearn.base import clone
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss
from sklearn.neural_network import MLPClassifier
from pathlib import Path

# ===== íŒŒë¼ë¯¸í„° / ì„¤ì • =====
EXP_NAME        = os.getenv("MLFLOW_EXPERIMENT_NAME", "iris-rf")
RUN_NAME        = (os.getenv("GIT_SHA", "")[:12] or "run")

# ê¸°ë³¸ ëŸ¬ë‹íƒ€ì„ íƒ€ê¹ƒ(ì´ˆ) â€” ëŒ€ëµ 5ë¶„ (ìš”ì²­ì‚¬í•­ ìœ ì§€)
TARGET_WALL_SEC = float(os.getenv("TARGET_WALL_SEC", "300"))

EPOCHS          = int(os.getenv("MLFLOW_EPOCHS", "40"))
BATCH_SIZE      = int(os.getenv("TRAIN_BATCH_SIZE", "64"))
SLEEP_SEC       = float(os.getenv("TRAIN_SLEEP_SEC", "0.0"))

# ğŸ”¸ í•™ìŠµ ì—°ì‚°ëŸ‰ ì œì–´ íŒŒë¼ë¯¸í„°
LOOPS_PER_EPOCH = int(os.getenv("LOOPS_PER_EPOCH", "3"))
AUGMENT_ENABLE  = os.getenv("AUGMENT_ENABLE", "1") == "1"
AUGMENT_COPIES  = int(os.getenv("AUGMENT_COPIES", "3"))
AUGMENT_NOISE   = float(os.getenv("AUGMENT_NOISE", "0.08"))

# ğŸ”¸ burn(ì¶”ê°€ ì—°ì‚°) - í•™ìŠµ ì™¸ì˜ ìˆœìˆ˜ ì—°ì‚°ìœ¼ë¡œ ì‹œê°„ì„ ë” ì“°ëŠ” ë¶€ë¶„
DEFAULT_BURN_PASSES = "1200"
BURN_PASSES     = int(os.getenv("BURN_PASSES", DEFAULT_BURN_PASSES))
BURN_NOISE      = float(os.getenv("BURN_NOISE", "0.08"))
BURN_ENABLE     = os.getenv("BURN_ENABLE", "1") == "1"
BURN_CHUNK_PASSES = int(os.getenv("BURN_CHUNK_PASSES", "256"))

# MLP í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë¬´ê²ê²Œ ë§Œë“¤ í¬ì¸íŠ¸)
HIDDEN_WIDTH    = int(os.getenv("MLP_HIDDEN_WIDTH", "256"))
HIDDEN_LAYERS   = int(os.getenv("MLP_HIDDEN_LAYERS", "3"))
LR_INITIAL      = float(os.getenv("LR_INITIAL", "0.01"))
RANDOM_STATE    = int(os.getenv("SEED", "42"))
EMA_ALPHA       = float(os.getenv("ETA_EMA_ALPHA", "0.2"))

# ğŸ”¸ ë¡œê·¸ ì„¤ì •: Promtail/Lokiìš© JSON ë¡œê·¸ ì¶œë ¥ (ê¸°ë³¸ ON)
LOG_JSON        = os.getenv("LOG_JSON", "1") == "1"

def log_json_line(payload: dict):
    """í•œ ì¤„ JSON ë¡œê·¸ ì¶œë ¥ (stdout). Promtailì´ ìˆ˜ì§‘í•´ Lokië¡œ ë³´ëƒ„."""
    if not LOG_JSON:
        return
    try:
        print(json.dumps(payload, separators=(",", ":"), ensure_ascii=False))
    except Exception:
        pass

def ensure_experiment_id(name: str, client: MlflowClient, retries: int = 20, sleep: float = 0.25) -> str:
    exp = client.get_experiment_by_name(name)
    if exp is None:
        exp_id = None
        try:
            exp_id = client.create_experiment(name)
        except RestException as e:
            if "RESOURCE_ALREADY_EXISTS" in str(e) or "UNIQUE constraint failed" in str(e):
                exp = client.get_experiment_by_name(name)
                if exp is not None:
                    exp_id = exp.experiment_id
            else:
                raise
        if exp_id is None:
            for i in range(retries):
                exp = client.get_experiment_by_name(name)
                if exp is not None:
                    exp_id = exp.experiment_id
                    break
                time.sleep(sleep * (1.5 ** i))
            if exp_id is None:
                raise RuntimeError(f"Failed to ensure experiment '{name}' exists")
    else:
        exp_id = exp.experiment_id

    for i in range(retries):
        try:
            if client.get_experiment(exp_id) is not None:
                return exp_id
        except RestException:
            pass
        time.sleep(sleep * (1.5 ** i))

    exp = client.get_experiment_by_name(name)
    if exp:
        return exp.experiment_id
    raise RuntimeError(f"Experiment id for '{name}' could not be validated")

def start_run_with_retry(exp_id: str, run_name: str, retries: int = 12, delay: float = 0.3, backoff: float = 1.5):
    last = None
    for _ in range(retries):
        try:
            return mlmod.start_run(experiment_id=exp_id, run_name=run_name)
        except RestException as e:
            last = e
            if "No Experiment with id" not in str(e) and "RESOURCE_DOES_NOT_EXIST" not in str(e):
                raise
            time.sleep(delay); delay *= backoff
    if last:
        raise last

def batch_iter(X, y, batch_size, shuffle=True, seed=None):
    n = len(X)
    idx = np.arange(n)
    if shuffle:
        rng = np.random.default_rng(seed)
        rng.shuffle(idx)
    for start in range(0, n, batch_size):
        end = min(start + batch_size, n)
        b = idx[start:end]
        yield X[b], y[b]

def extra_training_burn(template_clf, X, y, passes, batch_size, noise, seed):
    """
    ì¶”ê°€ í•™ìŠµ/ì—°ì‚° (ë¡œê·¸ ë°˜ì˜ X, pure compute).
    ì—¬ê¸°ì„œëŠ” template_clfì™€ ë¹„ìŠ·í•œ MLPë¥¼ ë³µì œí•´ì„œ
    ëœë¤ ë…¸ì´ì¦ˆ ë°°ì¹˜ë¡œ partial_fitì„ ì—¬ëŸ¬ ë²ˆ ëŒë ¤ì¤Œ.
    """
    if passes <= 0:
        return
    rng = np.random.default_rng(seed)
    shadow = clone(template_clf)
    classes = np.unique(y)

    # ì´ˆê¸° partial_fit (classes ì„¸íŒ…)
    if len(X) >= batch_size:
        xb = X[:batch_size]
        yb = y[:batch_size]
    else:
        xb = X
        yb = y
    shadow.partial_fit(xb, yb, classes=classes)

    n = len(X)
    for _ in range(passes):
        idx = rng.integers(0, n, size=min(batch_size, n))
        Xb = X[idx] + rng.normal(0.0, noise, size=X[idx].shape)
        shadow.partial_fit(Xb, y[idx])

def spend_time_to_target(template_clf, X, y, batch_size, noise, seed, target_deadline_sec):
    """
    ëª©í‘œ ë²½ì‹œê³„ ì‹œê°„ê¹Œì§€ ë‚¨ì€ ì‹œê°„ì„ íƒœìš°ê¸° ìœ„í•œ burn ë£¨í”„.
    """
    if not BURN_ENABLE:
        return
    start = time.perf_counter()
    rng_seed = seed
    while time.perf_counter() - start < target_deadline_sec:
        extra_training_burn(
            template_clf=template_clf,
            X=X, y=y,
            passes=BURN_CHUNK_PASSES,
            batch_size=batch_size,
            noise=noise,
            seed=rng_seed
        )
        rng_seed += 1

# === GHCR ë©”íƒ€ë°ì´í„°ë¥¼ MLflow íƒœê·¸/ì•„í‹°íŒ©íŠ¸ë¡œ ë‚¨ê¸°ëŠ” í—¬í¼ (ê·¸ëŒ€ë¡œ ìœ ì§€)
def log_ghcr_metadata_to_mlflow():
    """
    GHCR ê´€ë ¨ ENVë¥¼ ì½ì–´ MLflow íƒœê·¸ë¡œ ì €ì¥.
    ë˜í•œ ë™ì¼ ë‚´ìš©ì„ build/image.jsonìœ¼ë¡œ ê¸°ë¡í•´ ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ.
    """
    ghcr_image   = os.getenv("GHCR_IMAGE", "")
    ghcr_tag     = os.getenv("GHCR_TAG", "")
    ghcr_digest  = os.getenv("GHCR_DIGEST", "")
    ghcr_ref     = os.getenv("GHCR_IMAGE_REF", "")
    run_id_ci    = os.getenv("GITHUB_RUN_ID", "")
    git_sha      = os.getenv("GIT_SHA", "")

    # íƒœê·¸ ê¸°ë¡ (UI/ê²€ìƒ‰ìš©)
    mlmod.set_tag("ci.run_id", run_id_ci)
    mlmod.set_tag("git.sha",   git_sha)
    mlmod.set_tag("ghcr.image",  ghcr_image)
    mlmod.set_tag("ghcr.tag",    ghcr_tag)
    mlmod.set_tag("ghcr.digest", ghcr_digest)
    mlmod.set_tag("ghcr.ref",    ghcr_ref)

    # ì•„í‹°íŒ©íŠ¸ JSONë„ ë‚¨ê¹€
    try:
        meta = {
            "image": ghcr_image,
            "tag": ghcr_tag,
            "digest": ghcr_digest,
            "ref": ghcr_ref,
            "run_id": run_id_ci,
            "git_sha": git_sha
        }
        Path("build").mkdir(exist_ok=True)
        with open("build/image.json", "w") as f:
            json.dump(meta, f, indent=2)
        mlmod.log_artifact("build/image.json", artifact_path="build")
    except Exception:
        pass

def main():
    wall_start = time.perf_counter()

    # MLflow ì„¸íŒ…
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI") or os.getenv("TRACKING_URI") or mlmod.get_tracking_uri()
    mlmod.set_tracking_uri(tracking_uri)
    client = MlflowClient(tracking_uri=tracking_uri)
    exp_id = ensure_experiment_id(EXP_NAME, client)

    # ë°ì´í„° ë¡œë”©
    iris = load_iris()
    X_train, X_test, y_train, y_test = train_test_split(
        iris.data, iris.target, test_size=0.30, random_state=0
    )

    # ==== ë°ì´í„° ì¦ê°• (ì›ë˜ ë¡œì§ ìœ ì§€) ====
    if AUGMENT_ENABLE and AUGMENT_COPIES > 0:
        rng = np.random.default_rng(RANDOM_STATE + 777)
        X_aug_list = [X_train]; y_aug_list = [y_train]
        for _ in range(AUGMENT_COPIES):
            noise = rng.normal(0.0, AUGMENT_NOISE, size=X_train.shape)
            X_aug_list.append(X_train + noise)
            y_aug_list.append(y_train)
        X_train = np.vstack(X_aug_list)
        y_train = np.hstack(y_aug_list)

    classes = np.unique(y_train)

    # ===== MLP ëª¨ë¸ êµ¬ì„± (ë” ë¬´ê²ê²Œ) =====
    # hidden_layer_sizes = (HIDDEN_WIDTH, HIDDEN_WIDTH, ..., HIDDEN_WIDTH)
    hidden_layers = tuple([HIDDEN_WIDTH] * HIDDEN_LAYERS)

    # warm_start=True í•˜ë©´ partial_fitì²˜ëŸ¼ ëˆ„ì í•™ìŠµ ëŠë‚Œì„ ì¤„ ìˆ˜ëŠ” ì—†ì§€ë§Œ,
    # ìš°ë¦¬ëŠ” ì•„ë˜ì—ì„œ ì§ì ‘ partial_fitì„ ì“¸ ê±°ë¼, solver='sgd'ë¡œ ì‘ì€ stepì”© ëŒë ¤ì„œ
    # ë°˜ë³µ ì—°ì‚°ëŸ‰ì„ ëŠ˜ë¦°ë‹¤.
    base_clf = MLPClassifier(
        hidden_layer_sizes=hidden_layers,
        activation="relu",
        solver="sgd",
        learning_rate_init=LR_INITIAL,
        momentum=0.9,
        n_iter_no_change=200,      # ìš°ë¦¬ëŠ” ëŠì–´ì„œ partial_fit ë¹„ìŠ·í•˜ê²Œ ë•Œë¦¼ â†’ early stop ì•ˆ ê±¸ë¦¬ê²Œ í¬ê²Œ
        max_iter=1,                # í•œ ë²ˆì— í•œ ìŠ¤í…ë§Œ ëŒë¦¬ê²Œ
        random_state=RANDOM_STATE,
        warm_start=False,
        tol=None,
        alpha=0.0001,
        shuffle=False,
        verbose=False
    )

    with start_run_with_retry(exp_id, RUN_NAME) as run:
        run_id = run.info.run_id
        print(f"[mlflow] run_id={run_id}, exp_id={exp_id}")

        # GHCR ë©”íƒ€ë°ì´í„° ê¸°ë¡ (ìœ ì§€)
        log_ghcr_metadata_to_mlflow()

        # MLflow íŒŒë¼ë¯¸í„° ê¸°ë¡ (í˜•ì‹ ìœ ì§€í•˜ë˜ ëª¨ë¸ ì„¤ëª…ë§Œ ë°”ê¿ˆ)
        mlmod.log_params({
            "model": f"MLPClassifier(hidden_layers={hidden_layers}, sgd)",
            "epochs": EPOCHS,
            "batch_size": BATCH_SIZE,
            "sleep_sec": SLEEP_SEC,
            "loops_per_epoch": LOOPS_PER_EPOCH,
            "burn_passes": BURN_PASSES,
            "augment": AUGMENT_ENABLE,
            "augment_copies": AUGMENT_COPIES,
            "augment_noise": AUGMENT_NOISE,
            "target_wall_sec": TARGET_WALL_SEC,
            "burn_chunk_passes": BURN_CHUNK_PASSES
        })

        # MLPClassifierëŠ” partial_fitì„ ì§€ì›í•˜ì§€ë§Œ,
        # multi_class ë¬¸ì œì—ì„œ classes ì¸ìë¥¼ ì²« í˜¸ì¶œì— ë„£ì–´ì¤˜ì•¼ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘.
        clf = clone(base_clf)
        # ì²« partial_fitìœ¼ë¡œ í´ë˜ìŠ¤ ìŠ¤í™ ê³ ì •
        init_batch_X = X_train[:min(BATCH_SIZE, len(X_train))]
        init_batch_y = y_train[:min(BATCH_SIZE, len(y_train))]
        clf.partial_fit(init_batch_X, init_batch_y, classes=classes)

        f1_hist = []
        ema = None  # epoch compute time ì§€ìˆ˜í‰í™œ

        for epoch in range(1, EPOCHS + 1):
            t_epoch = time.perf_counter()

            # ë³¸ í•™ìŠµ ë£¨í”„: ì—¬ëŸ¬ loop/batchë¡œ partial_fit ë°˜ë³µ
            for loop in range(LOOPS_PER_EPOCH):
                for Xb, yb in batch_iter(
                    X_train, y_train,
                    BATCH_SIZE,
                    shuffle=True,
                    seed=RANDOM_STATE + epoch * 1000 + loop
                ):
                    # ì—¬ëŸ¬ ë²ˆ partial_fit ëŒë¦¬ë©´ MLPê°€ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ë©´ì„œ
                    # CPU/GPU ì‹œê°„ì„ ì ì  ë” íƒœì›€ â†’ ë¬´ê±°ìš´ í•™ìŠµ íš¨ê³¼.
                    clf.partial_fit(Xb, yb)

            # burn ë‹¨ê³„: ì¶”ê°€ ì—°ì‚°ëŸ‰
            if BURN_ENABLE and BURN_PASSES > 0:
                extra_training_burn(
                    template_clf=clf,
                    X=X_train, y=y_train,
                    passes=BURN_PASSES,
                    batch_size=BATCH_SIZE,
                    noise=BURN_NOISE,
                    seed=RANDOM_STATE + 1000 + epoch
                )

            compute_sec = time.perf_counter() - t_epoch

            # === í‰ê°€/ë¡œê·¸ (í˜•ì‹ ë™ì¼) ===
            y_pred = clf.predict(X_test)
            acc = float(accuracy_score(y_test, y_pred))
            f1  = float(f1_score(y_test, y_pred, average="macro"))
            try:
                y_proba = clf.predict_proba(X_test)
                ll = float(log_loss(y_test, y_proba))
            except Exception:
                ll = float("nan")

            # ETA ì¶”ì •ìš© EMA
            if ema is None:
                ema = compute_sec
            else:
                ema = EMA_ALPHA * compute_sec + (1 - EMA_ALPHA) * ema

            elapsed = time.perf_counter() - wall_start
            eta_sec = max(0.0, TARGET_WALL_SEC - elapsed)

            # MLflow metrics ë¡œê·¸ (í‚¤ ì´ë¦„ ê·¸ëŒ€ë¡œ ìœ ì§€)
            mlmod.log_metrics({
                "accuracy": acc,
                "f1_score": f1,
                "log_loss": ll,
                "epoch_compute_sec": compute_sec,
                "epoch_sleep_sec": SLEEP_SEC,
                "epoch_time_sec": compute_sec + SLEEP_SEC,
                "eta_sec": eta_sec,
                "progress_pct": min(99.9, 100.0 * epoch / EPOCHS),
                "elapsed_sec": elapsed
            }, step=epoch)

            f1_hist.append(f1)

            # stdout JSON ë¡œê·¸ë„ ê¸°ì¡´ í¬ë§· ìœ ì§€ (Promtail/Loki ì†Œë¹„ í¬ë§·)
            print(
                f"[epoch {epoch:03d}] acc={acc:.4f} f1={f1:.4f} comp={compute_sec:.2f}s "
                f"sleep={SLEEP_SEC:.2f}s elapsed={elapsed:.1f}s ETA~{eta_sec:.1f}s"
            )
            log_json_line({
                "event": "epoch_metric",
                "epoch": epoch,
                "accuracy": acc,
                "duration": round(compute_sec + SLEEP_SEC, 4),
                "remaining_sec": round(eta_sec, 1),
                "run_id": run_id,
                "experiment": EXP_NAME,
            })

            # epoch ëì—ì„œ sleep (ì›ë˜ ë¡œì§ ìœ ì§€ ê°€ëŠ¥)
            if SLEEP_SEC > 0:
                time.sleep(SLEEP_SEC)

            # ì¶”ê°€ ì‹œê°„ ì±„ìš°ê¸°: ëª©í‘œ ë²½ì‹œê°„ì— ë§ì¶”ê¸°
            remaining_to_target = TARGET_WALL_SEC - (time.perf_counter() - wall_start)
            if BURN_ENABLE and remaining_to_target > 5.0:
                per_epoch_cap = float(os.getenv("PER_EPOCH_SPEND_CAP_SEC", "60"))
                to_spend = min(per_epoch_cap, max(0.0, remaining_to_target * 0.4))
                if to_spend > 1.0:
                    spend_time_to_target(
                        template_clf=clf,
                        X=X_train, y=y_train,
                        batch_size=BATCH_SIZE,
                        noise=BURN_NOISE,
                        seed=RANDOM_STATE + 2000 + epoch,
                        target_deadline_sec=to_spend
                    )

            # ì „ì²´ íƒ€ê¹ƒ ì‹œê°„(ê¸°ë³¸ 300ì´ˆ) ì±„ì› ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (ì›ë˜ ë¡œì§ ìœ ì§€)
            if time.perf_counter() - wall_start >= TARGET_WALL_SEC:
                print(f"[info] target wall time ({TARGET_WALL_SEC:.0f}s) reached. stopping early.")
                break

        # ì´ í•™ìŠµ ì‹œê°„ ê¸°ë¡ (ì›ë˜ ë©”íŠ¸ë¦­ í‚¤ ìœ ì§€)
        total_time = time.perf_counter() - wall_start
        mlmod.log_metric("train_time_total_sec", total_time)

        # ===== ì•„í‹°íŒ©íŠ¸ =====
        cm = confusion_matrix(y_test, clf.predict(X_test))
        plt.figure(figsize=(5, 4))
        im = plt.imshow(cm, interpolation="nearest")
        plt.title("Confusion Matrix")
        plt.colorbar(im)
        tick = np.arange(len(iris.target_names))
        plt.xticks(tick, iris.target_names, rotation=45)
        plt.yticks(tick, iris.target_names)
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                plt.text(j, i, cm[i, j], ha="center", va="center")
        plt.xlabel("Predicted"); plt.ylabel("True"); plt.tight_layout()
        plt.savefig("confusion_matrix.png", bbox_inches="tight")
        mlmod.log_artifact("confusion_matrix.png", artifact_path="plots")

        plt.figure(figsize=(6, 3.5))
        plt.plot(range(1, len(f1_hist)+1), f1_hist, marker="o")
        plt.title("F1 over Epochs")
        plt.xlabel("Epoch"); plt.ylabel("F1 (macro)"); plt.grid(True, alpha=0.3)
        plt.savefig("learning_curve_f1.png", bbox_inches="tight")
        mlmod.log_artifact("learning_curve_f1.png", artifact_path="plots")

        # ëª¨ë¸ ì €ì¥: sklearn ëª¨ë¸ë¡œì„œ log_model (= ë°”ê¹¥ promote íŒŒì´í”„ë¼ì¸ ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ìœ ì§€)
        from mlflow import sklearn as ml_sklearn
        signature = infer_signature(X_train, clf.predict(X_train))
        ml_sklearn.log_model(
            sk_model=clf,
            artifact_path="model",
            signature=signature,
            input_example=X_train[:2]
        )

        # ì˜ˆì œ ì¸í’‹ë„ ê·¸ëŒ€ë¡œ ë‚¨ê¹€
        with open("input_example.json", "w") as f:
            json.dump(X_test[:2].tolist(), f)
        mlmod.log_artifact("input_example.json")

        # ë§ˆì§€ë§‰ JSON ë¡œê·¸ë„ ê¸°ì¡´ í¬ë§· ê·¸ëŒ€ë¡œ ìœ ì§€
        final_acc = float(f1_hist[-1]) if f1_hist else float("nan")
        log_json_line({
            "event": "train_done",
            "accuracy": float(accuracy_score(y_test, clf.predict(X_test))),
            "duration": round(total_time, 4),
            "remaining_sec": 0.0,
            "run_id": run_id,
            "experiment": EXP_NAME,
        })

        print("âœ… Train done.")

if __name__ == "__main__":
    main()

