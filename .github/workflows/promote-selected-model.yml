name: Promote-Selected-Model

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: "MLflow model name (e.g. light-logreg)"
        required: true
        default: "light-logreg"
      run_id:
        description: "MLflow RUN_ID of the training run (optional; empty=use latest version)"
        required: false
        default: ""
      serve_stage:
        description: "Stage to serve (e.g., Production)"
        required: true
        default: "Production"

permissions:
  contents: read
  actions: read

jobs:
  promote:
    runs-on: self-hosted

    env:
      #################################################################
      # --- cluster / deployment config ---
      #################################################################
      NAMESPACE: mlops
      SERVE_DEPLOY: light-serve
      SERVE_CONTAINER_INDEX: "0"
      SERVE_PORT: "8000"

      #################################################################
      # --- MLflow / registry info ---
      #
      # IMPORTANT:
      # runner 파드에서 mlflow ClusterIP:5000은 막혔고,
      # mlflow NodePort 32055 는 붙는 것 확인했지?
      # node2.example.com INTERNAL-IP = 10.100.0.102
      # => 이걸 트래킹 URI로 쓴다.
      #################################################################
      MLFLOW_TRACKING_URI: http://10.100.0.102:32055

      MODEL_NAME: ${{ inputs.model_name }}
      RUN_ID: ${{ inputs.run_id }}          # 이제 선택값 (빈 값 허용)
      SERVE_STAGE: ${{ inputs.serve_stage }}

      #################################################################
      # --- MinIO / S3 creds for mlflow.register_model() in runner ---
      #################################################################
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_DEFAULT_REGION: us-east-1
      AWS_S3_FORCE_PATH_STYLE: "true"
      AWS_S3_SIGNATURE_VERSION: s3v4
      AWS_S3_ADDRESSING_STYLE: path
      MLFLOW_S3_ENDPOINT_URL: http://10.100.0.101:32091
      MLFLOW_ARTIFACT_ROOT: s3://mlflow

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps (runner side)
        run: |
          set -euo pipefail
          pip install --no-cache-dir \
            mlflow-skinny \
            boto3 \
            kubernetes \
            pyyaml

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.30.0"

      - name: Build in-cluster kubeconfig
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
        run: |
          set -euo pipefail
          CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          SA_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
          API="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"

          kubectl config set-cluster in-cluster \
            --server="${API}" \
            --certificate-authority="${CACERT}" \
            --embed-certs=true

          kubectl config set-credentials sa --token="${SA_TOKEN}"

          kubectl config set-context in-cluster \
            --cluster=in-cluster \
            --user=sa \
            --namespace="${NAMESPACE}"

          kubectl config use-context in-cluster

      #################################################################
      # RUN_ID가 비었으면 최신 모델 버전을, 채워졌으면 해당 RUN의 모델을 등록/찾아 승격
      #################################################################
      - name: Promote MLflow model/version (inline)
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MODEL_NAME: ${{ env.MODEL_NAME }}
          RUN_ID: ${{ env.RUN_ID }}
          SERVE_STAGE: ${{ env.SERVE_STAGE }}
        run: |
          set -euo pipefail
          python - << 'PYCODE'
          import os, mlflow
          from mlflow.tracking import MlflowClient
          from mlflow.exceptions import MlflowException

          tracking_uri = os.environ["MLFLOW_TRACKING_URI"]
          model_name   = os.environ["MODEL_NAME"]
          run_id       = (os.environ.get("RUN_ID") or "").strip()
          stage        = os.environ["SERVE_STAGE"]

          mlflow.set_tracking_uri(tracking_uri)
          client = MlflowClient(tracking_uri=tracking_uri)

          version_num = None

          if run_id:
              model_uri = f"runs:/{run_id}/model"
              print(f"[promote] registering {model_uri} -> {model_name}", flush=True)
              try:
                  mv = mlflow.register_model(model_uri=model_uri, name=model_name)
                  version_num = mv.version
                  print(f"[promote] registered new version={version_num}", flush=True)
              except MlflowException as e:
                  print(f"[promote] register_model exception: {e}", flush=True)
                  print("[promote] trying to find existing model version for this run_id ...", flush=True)
                  for mv in client.search_model_versions(f"name='{model_name}'"):
                      if mv.run_id == run_id:
                          version_num = mv.version
                          print(f"[promote] found existing version={version_num} for run_id", flush=True)
                          break

          # run_id가 비어있거나 위 로직에서 version을 못 구한 경우 → 최신 버전 사용
          if version_num is None:
              all_versions = client.search_model_versions(f"name='{model_name}'")
              if all_versions:
                  all_versions = sorted(all_versions, key=lambda m:int(m.version), reverse=True)
                  version_num = all_versions[0].version
                  print(f"[promote] fallback using latest version={version_num}", flush=True)
              else:
                  raise RuntimeError("No model version could be determined at all")

          print(f"[promote] transition v{version_num} -> stage={stage}", flush=True)
          try:
              client.transition_model_version_stage(
                  name=model_name,
                  version=version_num,
                  stage=stage,
                  archive_existing_versions=False,
              )
          except MlflowException as e:
              print(f"[promote] transition warning: {e}", flush=True)

          print("[promote] done.", flush=True)
          PYCODE

      #################################################################
      # 5. Resolve GHCR image info from MLflow run tags
      #################################################################
      - name: Resolve runtime image from MLflow run tags
        id: resolve_image
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -euo pipefail
          python - << 'PYCODE'
          import os, mlflow

          tracking_uri = os.environ["MLFLOW_TRACKING_URI"]
          run_id = (os.environ.get("RUN_ID") or "").strip()

          mlflow.set_tracking_uri(tracking_uri)

          serve_image = ""
          reason = "fallback-none"

          if run_id:
              try:
                  run = mlflow.get_run(run_id)
                  tags = run.data.tags or {}
                  ref     = (tags.get("ghcr.ref") or "").strip()
                  image   = (tags.get("ghcr.image") or "").strip()
                  digest  = (tags.get("ghcr.digest") or "").strip()
                  tag     = (tags.get("ghcr.tag") or "").strip()

                  if ref:
                      serve_image = ref; reason = "ghcr.ref"
                  elif image and digest:
                      serve_image = f"{image}@{digest}"; reason = "image+digest"
                  elif image and tag:
                      serve_image = f"{image}:{tag}"; reason = "image+tag"
              except Exception as e:
                  print(f"[resolve_image] warn: {e}")

          print(f"[resolve_image] RUN_ID={run_id or '<empty>'}", flush=True)
          print(f"[resolve_image] SERVE_IMAGE={serve_image or '<empty>'} ({reason})", flush=True)

          with open(os.environ["GITHUB_ENV"], "a") as f:
              f.write(f"SERVE_IMAGE={serve_image}\n")
              f.write(f"SERVE_IMAGE_REASON={reason}\n")
          PYCODE

      - name: Debug resolved image (optional)
        run: |
          echo "[info] SERVE_IMAGE=${SERVE_IMAGE:-<empty>} (reason=${SERVE_IMAGE_REASON:-<none>})"
          if [ -z "${SERVE_IMAGE:-}" ]; then
            echo "::warning ::Could not resolve SERVE_IMAGE from MLflow tags; will reuse current Deployment image."
          fi

      - name: Ensure MLflow/MinIO env on Deployment
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
          SERVE_DEPLOY: ${{ env.SERVE_DEPLOY }}
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MLFLOW_S3_ENDPOINT_URL: ${{ env.MLFLOW_S3_ENDPOINT_URL }}
          AWS_DEFAULT_REGION: ${{ env.AWS_DEFAULT_REGION }}
          AWS_S3_FORCE_PATH_STYLE: ${{ env.AWS_S3_FORCE_PATH_STYLE }}
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" set env deploy/${SERVE_DEPLOY} \
            MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
            MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL} \
            AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \
            AWS_S3_FORCE_PATH_STYLE=${AWS_S3_FORCE_PATH_STYLE} \
            AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
            AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
            AWS_S3_SIGNATURE_VERSION=${AWS_S3_SIGNATURE_VERSION} \
            AWS_S3_ADDRESSING_STYLE=${AWS_S3_ADDRESSING_STYLE} \
            || true

      - name: Reconcile Deployment and restart fresh pod
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
          SERVE_DEPLOY: ${{ env.SERVE_DEPLOY }}
          SERVE_CONTAINER_INDEX: ${{ env.SERVE_CONTAINER_INDEX }}
          SERVE_PORT: ${{ env.SERVE_PORT }}
          MODEL_NAME: ${{ env.MODEL_NAME }}
          SERVE_STAGE: ${{ env.SERVE_STAGE }}
          SERVE_IMAGE: ${{ env.SERVE_IMAGE }}
        run: |
          set -euo pipefail

          echo "[reconcile] scale ${SERVE_DEPLOY} -> 0"
          kubectl -n "${NAMESPACE}" scale deploy "${SERVE_DEPLOY}" --replicas=0

          CMD="mlflow models serve -m models:/${MODEL_NAME}/${SERVE_STAGE} --host 0.0.0.0 --port ${SERVE_PORT} --env-manager local"
          echo "[reconcile] new CMD: ${CMD}"

          kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[ \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/command\",\"value\":[\"sh\",\"-c\"]}, \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/args\",\"value\":[\"${CMD}\"]}, \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/ports\",\"value\":[{\"containerPort\": ${SERVE_PORT}, \"name\": \"http\"}]}, \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/resources\",\"value\":{\"requests\": {\"cpu\": \"200m\", \"memory\": \"512Mi\"},\"limits\": {\"cpu\": \"1\", \"memory\": \"1Gi\"}}}, \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/startupProbe\",\"value\":{\"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},\"initialDelaySeconds\": 30,\"periodSeconds\": 5,\"timeoutSeconds\": 3,\"failureThreshold\": 120,\"successThreshold\": 1}}, \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/readinessProbe\",\"value\":{\"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},\"periodSeconds\": 5,\"timeoutSeconds\": 2,\"failureThreshold\": 6,\"successThreshold\": 1}}, \
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/livenessProbe\",\"value\":{\"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},\"initialDelaySeconds\": 60,\"periodSeconds\": 10,\"timeoutSeconds\": 2,\"failureThreshold\": 3,\"successThreshold\": 1}} \
          ]"

          if [ -n "${SERVE_IMAGE:-}" ]; then
            echo "[reconcile] patching image -> ${SERVE_IMAGE}"
            kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[ \
              {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/image\",\"value\":\"${SERVE_IMAGE}\"} \
            ]"
          else
            echo "::warning ::no SERVE_IMAGE resolved, keeping existing image"
          fi

          TS=$(date +%s)
          echo "[reconcile] bump workflows/restartedAt=${TS}"
          set +e
          kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[ \
            {\"op\":\"replace\",\"path\":\"/spec/template/metadata/annotations/workflows~1restartedAt\",\"value\":\"${TS}\"} \
          ]"
          RC=$?
          set -e
          if [ $RC -ne 0 ]; then
            echo "[reconcile] replace failed, doing add instead"
            kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[ \
              {\"op\":\"add\",\"path\":\"/spec/template/metadata/annotations/workflows~1restartedAt\",\"value\":\"${TS}\"} \
            ]"
          fi

          echo "[reconcile] scale ${SERVE_DEPLOY} -> 1"
          kubectl -n "${NAMESPACE}" scale deploy "${SERVE_DEPLOY}" --replicas=1

      - name: Wait for rollout
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
          SERVE_DEPLOY: ${{ env.SERVE_DEPLOY }}
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" rollout status deploy/${SERVE_DEPLOY} --timeout=300s
          echo "[done] final pods:"
          kubectl -n "${NAMESPACE}" get pods -l app=${SERVE_DEPLOY} -o wide

