name: Promote-Selected-Model

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: "MLflow model name (e.g., light-logreg)"
        required: true
        default: "light-logreg"
      run_id:
        description: "RUN_ID to promote (copy from MLflow UI)"
        required: true
      serve_stage:
        description: "MLflow stage to serve (ex: Production)"
        required: true
        default: "Production"

jobs:
  promote:
    runs-on: self-hosted

    env:
      # --- cluster / deployment config ---
      NAMESPACE: mlops
      SERVE_DEPLOY: light-serve
      SERVE_CONTAINER_INDEX: "0"
      SERVE_PORT: "8000"

      # --- MLflow / registry ---
      MLFLOW_TRACKING_URI: http://mlflow.mlops.svc.cluster.local:5000
      MODEL_NAME: ${{ inputs.model_name }}
      RUN_ID: ${{ inputs.run_id }}
      SERVE_STAGE: ${{ inputs.serve_stage }}

    steps:
      # ---- 0. repo checkout (need promote.py) ----
      - uses: actions/checkout@v4

      # ---- 1. Python runtime for mlflow, kubernetes client, etc. ----
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps (runner side)
        run: |
          pip install --no-cache-dir \
            mlflow-skinny \
            boto3 \
            kubernetes \
            pyyaml \
            jq

      # ---- 2. kubectl binary ----
      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.30.0'

      # ---- 3. in-cluster kubeconfig (runner is in cluster w/ SA token) ----
      - name: Build in-cluster kubeconfig
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
        run: |
          set -e
          CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          SA_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
          API="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"

          kubectl config set-cluster in-cluster \
            --server="${API}" \
            --certificate-authority="${CACERT}" \
            --embed-certs=true

          kubectl config set-credentials sa --token="${SA_TOKEN}"

          kubectl config set-context in-cluster \
            --cluster=in-cluster \
            --user=sa \
            --namespace="${NAMESPACE}"

          kubectl config use-context in-cluster

      # ---- 4. Promote the given RUN_ID into the requested stage in MLflow Registry ----
      # promote.py is assumed to:
      #   - take env RUN_ID, MODEL_NAME, SERVE_STAGE, etc.
      #   - register that run's artifacts as a model version
      #   - transition that version to SERVE_STAGE (e.g. Production)
      - name: Promote in MLflow (runs:/<RUN_ID>/model -> $SERVE_STAGE)
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MODEL_NAME: ${{ env.MODEL_NAME }}
          RUN_ID: ${{ env.RUN_ID }}
          SERVE_STAGE: ${{ env.SERVE_STAGE }}
        run: |
          set -euo pipefail
          python promote.py

      # ---- 5. Resolve GHCR runtime image for this RUN_ID from MLflow run tags ----
      # train.py was modified to log tags like:
      #   ghcr.ref       (full ref like ghcr.io/...:runtime-<hash> OR ghcr.io/...@sha256:...)
      #   ghcr.image     (ghcr.io/nacgyun/mlopsgit:runtime)
      #   ghcr.digest    (sha256:....)
      #   ghcr.tag       (runtime-<sha>...)
      #
      # We use best available to build SERVE_IMAGE. This gets exported to GITHUB_ENV
      - name: Resolve GHCR image from RUN tags
        id: resolve_image
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -euo pipefail
          python - << 'PYCODE'
          import os, sys, mlflow

          tracking_uri = os.environ["MLFLOW_TRACKING_URI"]
          run_id = os.environ["RUN_ID"]

          mlflow.set_tracking_uri(tracking_uri)
          run = mlflow.get_run(run_id)
          tags = run.data.tags or {}

          ref     = (tags.get("ghcr.ref") or "").strip()
          image   = (tags.get("ghcr.image") or "").strip()
          digest  = (tags.get("ghcr.digest") or "").strip()
          tag     = (tags.get("ghcr.tag") or "").strip()

          serve_image = ""
          reason = ""

          if ref:
              serve_image = ref
              reason = "ghcr.ref"
          elif image and digest:
              serve_image = f"{image}@{digest}"
              reason = "image+digest"
          elif image and tag:
              serve_image = f"{image}:{tag}"
              reason = "image+tag"
          else:
              serve_image = ""
              reason = "fallback-none"

          print(f"[resolve_image] RUN_ID={run_id} SERVE_IMAGE={serve_image} source={reason}", file=sys.stderr)

          with open(os.environ["GITHUB_ENV"], "a") as f:
              f.write(f"SERVE_IMAGE={serve_image}\n")
              f.write(f"SERVE_IMAGE_REASON={reason}\n")
          PYCODE

          echo "Resolved SERVE_IMAGE=$SERVE_IMAGE (reason=$SERVE_IMAGE_REASON)"
          if [ -z "$SERVE_IMAGE" ]; then
            echo "::warning ::Failed to resolve SERVE_IMAGE from MLflow run tags. Will keep current Deployment image."
          fi

      # ---- 6. Make sure light-serve Deployment has MinIO/MLflow env ----
      # This guarantees the container knows how to talk to MLflow tracking server and MinIO.
      - name: Ensure MLflow/MinIO env on Deployment
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
          SERVE_DEPLOY: ${{ env.SERVE_DEPLOY }}
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" set env deploy/${SERVE_DEPLOY} \
            MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI} \
            MLFLOW_S3_ENDPOINT_URL=http://minio.${NAMESPACE}.svc.cluster.local:9000 \
            AWS_DEFAULT_REGION=us-east-1 \
            AWS_S3_FORCE_PATH_STYLE=true \
            || true
          # NOTE: We assume AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY are already
          #       present via secretRef in the Deployment, so we don't re-add here.

      # ---- 7. Patch Deployment spec with:
      #        - correct image (if SERVE_IMAGE resolved)
      #        - correct command/args pointing to models:/<MODEL_NAME>/<SERVE_STAGE>
      #        - consistent probes, resources, ports
      #        - workflows/restartedAt timestamp annotation to FORCE new ReplicaSet
      #
      # This guarantees that EVEN IF you deploy the same RUN_ID twice,
      # the annotation timestamp changes -> spec.template changes -> new RS -> new Pod.
      - name: Patch light-serve Deployment for new RUN (image + serve cmd + rollout bump)
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
          SERVE_DEPLOY: ${{ env.SERVE_DEPLOY }}
          SERVE_CONTAINER_INDEX: ${{ env.SERVE_CONTAINER_INDEX }}
          SERVE_PORT: ${{ env.SERVE_PORT }}
          MODEL_NAME: ${{ env.MODEL_NAME }}
          SERVE_STAGE: ${{ env.SERVE_STAGE }}
          SERVE_IMAGE: ${{ env.SERVE_IMAGE }}
        run: |
          set -euo pipefail

          # build runtime serve command, e.g.:
          # mlflow models serve -m models:/light-logreg/Production --host 0.0.0.0 --port 8000 --env-manager local
          CMD="mlflow models serve -m models:/${MODEL_NAME}/${SERVE_STAGE} --host 0.0.0.0 --port ${SERVE_PORT} --env-manager local"
          echo "[info] using CMD=${CMD}"
          echo "[info] SERVE_IMAGE=${SERVE_IMAGE}"

          # 7a. Patch core container config (command/args/ports/resources/probes)
          kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[
            {\"op\":\"replace\",\"path\":\"/spec/replicas\",\"value\":1},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/command\",
             \"value\":[\"sh\",\"-c\"]},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/args\",
             \"value\":[\"${CMD}\"]},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/ports\",
             \"value\":[{\"containerPort\": ${SERVE_PORT}, \"name\": \"http\"}]},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/resources\",
             \"value\":{
               \"requests\": {\"cpu\": \"200m\", \"memory\": \"512Mi\"},
               \"limits\":   {\"cpu\": \"1\",    \"memory\": \"1Gi\"}
             }},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/startupProbe\",
             \"value\":{
               \"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},
               \"initialDelaySeconds\": 30,
               \"periodSeconds\": 5,
               \"timeoutSeconds\": 3,
               \"failureThreshold\": 120,
               \"successThreshold\": 1
             }},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/readinessProbe\",
             \"value\":{
               \"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},
               \"periodSeconds\": 5,
               \"timeoutSeconds\": 2,
               \"failureThreshold\": 6,
               \"successThreshold\": 1
             }},

            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/livenessProbe\",
             \"value\":{
               \"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},
               \"initialDelaySeconds\": 60,
               \"periodSeconds\": 10,
               \"timeoutSeconds\": 2,
               \"failureThreshold\": 3,
               \"successThreshold\": 1
             }}
          ]"

          # 7b. Patch image if we resolved it from tags. If we couldn't, keep old image.
          if [ -n "${SERVE_IMAGE}" ]; then
            kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[
              {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${SERVE_CONTAINER_INDEX}/image\",
               \"value\":\"${SERVE_IMAGE}\"}
            ]"
          else
            echo "::warning ::SERVE_IMAGE empty -> keeping previous container image"
          fi

          # 7c. Force rollout by bumping an annotation with a fresh timestamp.
          TS=$(date +%s)
          kubectl -n "${NAMESPACE}" patch deploy "${SERVE_DEPLOY}" --type='json' -p="[
            {\"op\":\"add\",\"path\":\"/spec/template/metadata/annotations/workflows~1restartedAt\",
             \"value\":\"${TS}\"}
          ]"

      # ---- 8. Wait for rollout to become Ready (new pod should spin up) ----
      - name: Rollout status
        env:
          NAMESPACE: ${{ env.NAMESPACE }}
          SERVE_DEPLOY: ${{ env.SERVE_DEPLOY }}
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" rollout status deploy/${SERVE_DEPLOY} --timeout=300s

