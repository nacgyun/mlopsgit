name: Promote-Selected-Model

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: "MLflow model name (e.g., light-logreg)"
        required: true
        default: "light-logreg"
      run_id:
        description: "RUN_ID to promote (copy from MLflow UI)"
        required: true
      reload_method:
        description: "How to refresh serve (rollout|http)"
        required: true
        default: "rollout"

jobs:
  promote:
    runs-on: self-hosted

    env:
      # K8s config
      NAMESPACE: mlops
      SERVE_DEPLOY: light-serve
      SERVE_PORT: "8000"
      SERVE_STAGE: Production        # will serve models:/<MODEL_NAME>/<SERVE_STAGE>

      # MLflow config
      MLFLOW_TRACKING_URI: http://mlflow.mlops.svc.cluster.local:5000
      MODEL_NAME: ${{ inputs.model_name }}
      RUN_ID: ${{ inputs.run_id }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps (runner side)
        run: pip install --no-cache-dir mlflow-skinny

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.30.0'

      - name: Build in-cluster kubeconfig (runner pod)
        run: |
          set -e
          CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          SA_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
          API="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
          kubectl config set-cluster in-cluster --server="${API}" --certificate-authority="${CACERT}" --embed-certs=true
          kubectl config set-credentials sa --token="${SA_TOKEN}"
          kubectl config set-context in-cluster --cluster=in-cluster --user=sa --namespace=${NAMESPACE}
          kubectl config use-context in-cluster

      # 1) Promote the chosen RUN_ID into the registry (ModelVersion stage bump etc.)
      - name: Promote in MLflow (runs:/<RUN_ID>/model)
        run: python promote.py

      # 2) Resolve which GHCR image we should serve for this RUN_ID
      - name: Resolve GHCR image from MLflow RUN tags
        id: resolve_image
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, sys, mlflow
          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          run = mlflow.get_run(os.environ["RUN_ID"])
          tags = run.data.tags

          ref    = (tags.get("ghcr.ref")    or "").strip()
          image  = (tags.get("ghcr.image")  or "").strip()
          digest = (tags.get("ghcr.digest") or "").strip()
          tag    = (tags.get("ghcr.tag")    or "").strip()

          if ref:
              out, reason = ref, "ghcr.ref"
          elif image and digest:
              out, reason = f"{image}@{digest}", "image+digest"
          elif image and tag:
              out, reason = f"{image}:{tag}", "image+tag"
          else:
              out, reason = "", "fallback-none"

          print(f"[resolve_image] SERVE_IMAGE={out} reason={reason}", file=sys.stderr)

          with open(os.environ["GITHUB_ENV"], "a") as f:
              f.write(f"SERVE_IMAGE={out}\n")
              f.write(f"SERVE_IMAGE_REASON={reason}\n")
          PY

          if [ -z "${SERVE_IMAGE}" ]; then
            echo "::warning ::Could not resolve GHCR image for RUN_ID=${RUN_ID} from MLflow tags. We'll keep current image."
          else
            echo "Using SERVE_IMAGE=${SERVE_IMAGE} (source=${SERVE_IMAGE_REASON})"
          fi

      # 3) Normalize Deployment spec in ONE strategic merge patch:
      #    - replicas=1 (so we don't keep 2 pods from 2 ReplicaSets)
      #    - container 'server' runs mlflow models serve directly (no pip install, no exit-after-boot)
      #    - probes hit /ping on named port http
      #    - requests/limits sane
      #    - env includes: tracking URI, S3/MinIO, AWS creds from secret
      #
      #    NOTE: we inline the actual model URI ("models:/<MODEL_NAME>/<SERVE_STAGE>") directly
      #    into the serve command string. No "$(MODEL_URI)" indirection, no shell expansion required.
      - name: Normalize serve Deployment spec (strategic merge patch)
        run: |
          set -euo pipefail
          NS="${NAMESPACE}"
          DEPLOY="${SERVE_DEPLOY}"
          PORT="${SERVE_PORT}"
          MODEL_URI_VALUE="models:/${MODEL_NAME}/${SERVE_STAGE}"

          SERVE_CMD="mlflow models serve -m ${MODEL_URI_VALUE} --host 0.0.0.0 --port ${PORT} --env-manager local"

          kubectl -n "${NS}" patch deploy "${DEPLOY}" -p "{
            \"spec\": {
              \"replicas\": 1,
              \"template\": {
                \"spec\": {
                  \"containers\": [
                    {
                      \"name\": \"server\",
                      \"image\": \"ghcr.io/mlflow/mlflow:latest\",
                      \"command\": [\"sh\",\"-c\"],
                      \"args\": [\"${SERVE_CMD}\"],
                      \"ports\": [
                        {\"containerPort\": ${PORT}, \"name\": \"http\"}
                      ],
                      \"startupProbe\": {
                        \"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},
                        \"initialDelaySeconds\": 30,
                        \"periodSeconds\": 5,
                        \"timeoutSeconds\": 3,
                        \"failureThreshold\": 120
                      },
                      \"readinessProbe\": {
                        \"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},
                        \"periodSeconds\": 5,
                        \"timeoutSeconds\": 2,
                        \"failureThreshold\": 6
                      },
                      \"livenessProbe\": {
                        \"httpGet\": {\"path\": \"/ping\", \"port\": \"http\"},
                        \"initialDelaySeconds\": 60,
                        \"periodSeconds\": 10,
                        \"timeoutSeconds\": 2,
                        \"failureThreshold\": 3
                      },
                      \"resources\": {
                        \"requests\": {\"cpu\": \"200m\", \"memory\": \"512Mi\"},
                        \"limits\":   {\"cpu\": \"1\",    \"memory\": \"1Gi\"}
                      },
                      \"env\": [
                        {\"name\": \"MLFLOW_TRACKING_URI\",     \"value\": \"http://mlflow.${NS}.svc.cluster.local:5000\"},
                        {\"name\": \"MLFLOW_S3_ENDPOINT_URL\",  \"value\": \"http://minio.${NS}.svc.cluster.local:9000\"},
                        {\"name\": \"AWS_DEFAULT_REGION\",      \"value\": \"us-east-1\"},
                        {\"name\": \"AWS_S3_FORCE_PATH_STYLE\", \"value\": \"true\"},
                        {\"name\": \"AWS_ACCESS_KEY_ID\",
                         \"valueFrom\": {\"secretKeyRef\": {\"name\": \"minio-creds\", \"key\": \"accessKey\"}}},
                        {\"name\": \"AWS_SECRET_ACCESS_KEY\",
                         \"valueFrom\": {\"secretKeyRef\": {\"name\": \"minio-creds\", \"key\": \"secretKey\"}}}
                      ]
                    }
                  ]
                }
              }
            }
          }"

          echo "=== Deployment after normalize ==="
          kubectl -n "${NS}" get deploy "${DEPLOY}" -o yaml | sed -n '1,200p'

      # 4) If we successfully resolved SERVE_IMAGE from the RUN's MLflow tags,
      #    override the container image to that (immutable runtime image with all deps)
      - name: Patch serve image (from MLflow RUN tags)
        if: ${{ env.SERVE_IMAGE != '' }}
        run: |
          set -euo pipefail
          NS="${NAMESPACE}"
          DEPLOY="${SERVE_DEPLOY}"

          kubectl -n "${NS}" patch deploy "${DEPLOY}" -p "{
            \"spec\": {
              \"template\": {
                \"spec\": {
                  \"containers\": [
                    {
                      \"name\": \"server\",
                      \"image\": \"${SERVE_IMAGE}\"
                    }
                  ]
                }
              }
            }
          }"

          echo "Patched image to ${SERVE_IMAGE}"

      # 5) Force a new ReplicaSet every run (annotation bump),
      #    so old RS with old template won't keep a zombie pod.
      - name: Force template bump
        run: |
          set -e
          NS="${NAMESPACE}"
          DEPLOY="${SERVE_DEPLOY}"
          TS=$(date +%s)

          kubectl -n "${NS}" patch deploy "${DEPLOY}" -p "{
            \"spec\": {
              \"template\": {
                \"metadata\": {
                  \"annotations\": {
                    \"workflows/restartedAt\": \"${TS}\",
                    \"workflows/run-id\": \"${{ github.run_id }}\"
                  }
                }
              }
            }
          }"

          echo "=== Deployment after bump ==="
          kubectl -n "${NS}" get deploy "${DEPLOY}" -o wide

      # 6) Rollout & status dump
      - name: Rollout restart serve
        if: ${{ inputs.reload_method == 'rollout' || env.SERVE_IMAGE != '' }}
        run: |
          set -e
          NS="${NAMESPACE}"
          DEPLOY="${SERVE_DEPLOY}"

          kubectl -n "${NS}" rollout restart deploy/${DEPLOY} || true

          echo "=== Deployment ==="
          kubectl -n "${NS}" get deploy ${DEPLOY} -o wide

          echo "=== ReplicaSets ==="
          kubectl -n "${NS}" get rs -l app=${DEPLOY} -o wide || true

          echo "=== Pods (before wait) ==="
          kubectl -n "${NS}" get pods -l app=${DEPLOY} -o wide || true

          kubectl -n "${NS}" rollout status deploy/${DEPLOY} --timeout=600s || true

          echo "=== Pods (after wait) ==="
          kubectl -n "${NS}" get pods -l app=${DEPLOY} -o wide || true

      # 7) Optional HTTP hot-reload hook (if your serve container exposes /reload)
      - name: Call /reload on light-serve (HTTP) [optional]
        if: ${{ inputs.reload_method == 'http' }}
        run: |
          curl -s -X POST http://${{ env.SERVE_DEPLOY }}.${{ env.NAMESPACE }}.svc.cluster.local:${{ env.SERVE_PORT }}/reload || true

