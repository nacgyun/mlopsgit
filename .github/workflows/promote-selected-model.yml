name: Promote-Selected-Model

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: "MLflow model name (e.g., light-logreg)"
        required: true
        default: "light-logreg"
      run_id:
        description: "RUN_ID to promote (copy from MLflow UI)"
        required: true
      reload_method:
        description: "How to refresh serve (rollout|http)"
        required: true
        default: "rollout"

jobs:
  promote:
    runs-on: self-hosted

    env:
      # K8s
      NAMESPACE: mlops
      SERVE_DEPLOY: light-serve
      SERVE_CONTAINER_INDEX: "0"   # serve 컨테이너 인덱스
      SERVE_PORT: "8000"
      SERVE_STAGE: Production       # MODEL_URI는 models:/<MODEL_NAME>/<SERVE_STAGE>

      # MLflow
      MLFLOW_TRACKING_URI: http://mlflow.mlops.svc.cluster.local:5000
      MODEL_NAME: ${{ inputs.model_name }}
      RUN_ID: ${{ inputs.run_id }}

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps (runner side)
        run: pip install --no-cache-dir mlflow-skinny

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.30.0'

      - name: Build in-cluster kubeconfig (runner pod)
        run: |
          set -e
          CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          SA_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
          API="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
          kubectl config set-cluster in-cluster --server="${API}" --certificate-authority="${CACERT}" --embed-certs=true
          kubectl config set-credentials sa --token="${SA_TOKEN}"
          kubectl config set-context in-cluster --cluster=in-cluster --user=sa --namespace=${NAMESPACE}
          kubectl config use-context in-cluster

      # 1) 선택 RUN을 Registry로 승격 (기존 promote.py 유지 사용)
      - name: Promote in MLflow (runs:/<RUN_ID>/model)
        run: python promote.py

      # 2) MLflow RUN 태그에서 GHCR 이미지(ref 우선, 없으면 image+digest, 없으면 image:tag) 결정
      - name: Resolve GHCR image from MLflow RUN tags
        id: resolve_image
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          RUN_ID: ${{ env.RUN_ID }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, sys, mlflow
          mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
          run = mlflow.get_run(os.environ["RUN_ID"])
          tags = run.data.tags
          ref    = (tags.get("ghcr.ref")    or "").strip()
          image  = (tags.get("ghcr.image")  or "").strip()
          digest = (tags.get("ghcr.digest") or "").strip()
          tag    = (tags.get("ghcr.tag")    or "").strip()
          if ref:
              out, reason = ref, "ghcr.ref"
          elif image and digest:
              out, reason = f"{image}@{digest}", "image+digest"
          elif image and tag:
              out, reason = f"{image}:{tag}", "image+tag"
          else:
              out, reason = "", "fallback-none"
          print(f"Resolved={out} reason={reason}", file=sys.stderr)
          with open(os.environ["GITHUB_ENV"], "a") as f:
              f.write(f"SERVE_IMAGE={out}\n")
              f.write(f"SERVE_IMAGE_REASON={reason}\n")
          PY
          if [ -z "${SERVE_IMAGE}" ]; then
            echo "::warning ::Could not resolve GHCR image from MLflow tags for RUN_ID=${RUN_ID}. Skipping image patch."
          else
            echo "Using SERVE_IMAGE=${SERVE_IMAGE} (source=${SERVE_IMAGE_REASON})"
          fi

      # 3) (선택) MinIO/S3 환경만 보장 — 추가 설치/엔트리포인트 패치 없음
      - name: Ensure S3 env on serve
        run: |
          kubectl -n "${NAMESPACE}" set env deploy/${SERVE_DEPLOY} \
            MLFLOW_S3_ENDPOINT_URL=http://minio.${NAMESPACE}.svc.cluster.local:9000 \
            AWS_DEFAULT_REGION=us-east-1 \
            AWS_S3_FORCE_PATH_STYLE=true
          # Access key/secret은 이미 배포에 있으면 스킵
          # 필요 시:
          # kubectl -n "${NAMESPACE}" set env deploy/${SERVE_DEPLOY} \
          #   --from=secret/minio-creds --keys=accessKey,secretKey \
          #   --env-vars "AWS_ACCESS_KEY_ID=$(kubectl -n ${NAMESPACE} get secret minio-creds -o jsonpath='{.data.accessKey}' | base64 -d)" \
          #   "AWS_SECRET_ACCESS_KEY=$(kubectl -n ${NAMESPACE} get secret minio-creds -o jsonpath='{.data.secretKey}' | base64 -d)"

      # 4) MLflow 스테이지 URI 주입 (이미지의 기본 엔트리포인트가 MODEL_URI를 읽는다고 가정)
      - name: Set MODEL_URI env (stage)
        run: |
          kubectl -n "${NAMESPACE}" set env deploy/${SERVE_DEPLOY} \
            MODEL_URI="models:/${MODEL_NAME}/${SERVE_STAGE}"

      # 5) 결정한 GHCR 이미지로 Deployment 컨테이너 이미지 교체
      - name: Patch serve image (from MLflow RUN tags)
        if: ${{ env.SERVE_IMAGE != '' }}
        run: |
          set -e
          DEPLOY=$(kubectl -n "${NAMESPACE}" get deploy -o name | grep "${SERVE_DEPLOY}")
          INDEX=${SERVE_CONTAINER_INDEX:-0}
          kubectl -n "${NAMESPACE}" patch ${DEPLOY} --type='json' -p="[
            {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/${INDEX}/image\",\"value\":\"${SERVE_IMAGE}\"}
          ]"
          echo "Patched image to ${SERVE_IMAGE}"

      # 6) 롤링 재시작/상태 확인 (또는 http 핫리로드 사용 시 스킵)
      - name: Rollout restart serve
        if: ${{ inputs.reload_method == 'rollout' || env.SERVE_IMAGE != '' }}
        run: |
          set -e
          DEPLOY=$(kubectl -n "${NAMESPACE}" get deploy -o name | grep "${SERVE_DEPLOY}")
          kubectl -n "${NAMESPACE}" rollout restart "${DEPLOY}"
          kubectl -n "${NAMESPACE}" rollout status "${DEPLOY}" --timeout=600s

      # 7) (옵션) HTTP 핫 리로드 엔드포인트가 있다면
      - name: Call /reload on light-serve (HTTP) [optional]
        if: ${{ inputs.reload_method == 'http' }}
        run: |
          curl -s -X POST http://${{ env.SERVE_DEPLOY }}.${{ env.NAMESPACE }}.svc.cluster.local:${{ env.SERVE_PORT }}/reload || true

