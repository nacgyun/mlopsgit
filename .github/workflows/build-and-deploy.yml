name: build-and-deploy

on:
  push:
    branches: ["master"]
  workflow_dispatch:

permissions:
  contents: read
  packages: write

env:
  NAMESPACE: mlops
  APP_NAME: myapp
  IMAGE_REPO: "ghcr.io/${{ github.repository }}"
  IMAGE_TAG: "${{ github.sha }}"
  IMAGE: "ghcr.io/${{ github.repository }}:${{ github.sha }}"

jobs:
  build-and-deploy:
    runs-on: self-hosted

    steps:
      - uses: actions/checkout@v4

      - name: Install kubectl (no-sudo)
        run: |
          set -e
          ver=$(curl -Ls https://dl.k8s.io/release/stable.txt)
          curl -Ls -o "$RUNNER_TEMP/kubectl" "https://dl.k8s.io/release/${ver}/bin/linux/amd64/kubectl"
          chmod +x "$RUNNER_TEMP/kubectl"
          echo "$RUNNER_TEMP" >> "$GITHUB_PATH"

      - name: Configure kube access & verify
        run: |
          set -e
          echo "${{ secrets.KUBECONFIG }}" > "$RUNNER_TEMP/kubeconfig"
          echo "KUBECONFIG=$RUNNER_TEMP/kubeconfig" >> "$GITHUB_ENV"
          kubectl version --client
          kubectl get ns || true

      - name: Ensure namespace exists
        run: |
          kubectl get ns "$NAMESPACE" || kubectl create ns "$NAMESPACE"

      - name: Ensure GHCR docker secret
        run: |
          kubectl -n "$NAMESPACE" create secret docker-registry ghcr-creds \
            --docker-server=ghcr.io \
            --docker-username="${{ github.actor }}" \
            --docker-password="${{ secrets.GITHUB_TOKEN }}" \
            --docker-email="ci@example.local" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Cleanup previous Kaniko Job
        run: |
          kubectl -n "$NAMESPACE" delete job kaniko-build --ignore-not-found=true

      - name: Launch Kaniko Job
        run: |
          sed "s|__IMAGE__|${IMAGE}|g" k8s/kaniko-job.yaml.tmpl \
            | kubectl -n "$NAMESPACE" apply -f -

      # ✅ 스트리밍 업로드 + pipefail/타임아웃/준비확인 적용
      - name: Upload source (streaming) & release init
        run: |
          set -euo pipefail

          echo "== wait for kaniko pod =="
          for i in {1..60}; do
            POD=$(kubectl -n "$NAMESPACE" get pods -l job-name=kaniko-build -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
            [ -n "${POD:-}" ] && break
            sleep 2
          done
          echo "POD=${POD:-<none>}"
          [ -n "${POD:-}" ]

          echo "== wait until init container(fetch-src) is Running =="
          for i in {1..90}; do
            ICS=$(kubectl -n "$NAMESPACE" get pod "$POD" -o jsonpath='{.status.initContainerStatuses[?(@.name=="fetch-src")].state.running}')
            [ -n "${ICS:-}" ] && break
            PHASE=$(kubectl -n "$NAMESPACE" get pod "$POD" -o jsonpath='{.status.phase}')
            if [ "$PHASE" = "Failed" ]; then
              kubectl -n "$NAMESPACE" describe pod "$POD" || true
              exit 1
            fi
            sleep 2
          done
          echo "init container is running"

          echo "== sanity check =="
          kubectl -n "$NAMESPACE" exec "$POD" -c fetch-src -- sh -lc 'echo OK && uname -a'

          echo "== upload & extract =="
          tar -C "$GITHUB_WORKSPACE" -czf - . \
            | timeout 300 kubectl -n "$NAMESPACE" exec -i "$POD" -c fetch-src -- sh -lc 'set -euo pipefail; mkdir -p /workspace; rm -rf /workspace/*; tar -xzf - -C /workspace'

          echo "== ensure Dockerfile, then release =="
          kubectl -n "$NAMESPACE" exec "$POD" -c fetch-src -- sh -lc '
            set -e
            if [ ! -f /workspace/Dockerfile ]; then
              DF="$(find /workspace -maxdepth 2 -type f \( -name Dockerfile -o -name "Dockerfile.*" \) | head -n 1 || true)"
              if [ -n "$DF" ]; then
                cp "$DF" /workspace/Dockerfile
              else
                cat >/workspace/Dockerfile << "DF"
FROM python:3.11-slim
WORKDIR /app
COPY ml/ /app/
RUN if [ -f /app/requirements.txt ]; then pip install --no-cache-dir -r /app/requirements.txt; fi
ENV MLFLOW_EXPERIMENT_NAME=iris-rf
CMD ["python","-u","/app/train.py"]
DF
              fi
            fi
            ls -lah /workspace | head -n 50
            touch /workspace/.ready
          '

      - name: Wait for Kaniko build to finish (with diagnostics)
        run: |
          set -e
          if ! kubectl -n "$NAMESPACE" wait --for=condition=Complete job/kaniko-build --timeout=20m; then
            echo "== JOB FAILED: diagnostics =="
            kubectl -n "$NAMESPACE" get job/kaniko-build -o wide || true
            kubectl -n "$NAMESPACE" describe job/kaniko-build || true
            kubectl -n "$NAMESPACE" get pods -l job-name=kaniko-build -o wide || true
            POD=$(kubectl -n "$NAMESPACE" get pods -l job-name=kaniko-build -o jsonpath='{.items[0].metadata.name}')
            kubectl -n "$NAMESPACE" describe pod "$POD" || true
            echo "== INIT(fetch-src) tail =="
            kubectl -n "$NAMESPACE" logs "$POD" -c fetch-src --tail=200 || true
            echo "== KANIKO tail =="
            kubectl -n "$NAMESPACE" logs "$POD" -c kaniko --tail=200 || true
            exit 1
          fi

      - name: Run training Job (MLflow)
        run: |
          set -e
          cat > /tmp/iris-train.yaml <<'K8S'
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: iris-train
            labels: { app: iris-train }
          spec:
            ttlSecondsAfterFinished: 600
            backoffLimit: 0
            template:
              spec:
                restartPolicy: Never
                imagePullSecrets:
                  - name: ghcr-creds
                containers:
                  - name: train
                    image: __IMAGE__
                    imagePullPolicy: IfNotPresent
                    env:
                      - name: MLFLOW_TRACKING_URI
                        value: http://mlflow.mlops.svc.cluster.local:5000
                      - name: MLFLOW_EXPERIMENT_NAME
                        value: iris-rf
                      - name: MLFLOW_S3_ENDPOINT_URL
                        value: http://minio.mlops.svc.cluster.local:9000
                      - name: AWS_ACCESS_KEY_ID
                        valueFrom: { secretKeyRef: { name: minio-creds, key: accessKey } }
                      - name: AWS_SECRET_ACCESS_KEY
                        valueFrom: { secretKeyRef: { name: minio-creds, key: secretKey } }
                    resources:
                      requests: { cpu: "500m", memory: "1Gi" }
                      limits:   { cpu: "2",    memory: "4Gi" }
          K8S
          sed -i "s|__IMAGE__|${IMAGE}|g" /tmp/iris-train.yaml
          kubectl -n "$NAMESPACE" delete job iris-train --ignore-not-found
          kubectl -n "$NAMESPACE" apply -f /tmp/iris-train.yaml

      - name: Tail training logs (best-effort)
        continue-on-error: true
        run: |
          kubectl -n "$NAMESPACE" logs -f job/iris-train -c train --since=1s || true

      - name: Set image and rollout
        run: |
          set -e
          kubectl -n "$NAMESPACE" apply -f k8s/deploy.yaml
          kubectl -n "$NAMESPACE" set image deploy/${APP_NAME} ${APP_NAME}=${IMAGE} --record
          kubectl -n "$NAMESPACE" rollout status deploy/${APP_NAME} --timeout=5m

