name: build-and-deploy
on:
  push:
    branches: ["master"]
  workflow_dispatch:

permissions:
  contents: read
  packages: write

env:
  NAMESPACE: mlops
  APP_NAME: myapp
  IMAGE_REPO: ghcr.io/${{ github.repository }}
  IMAGE_TAG: ${{ github.sha }}
  IMAGE: ghcr.io/${{ github.repository }}:${{ github.sha }}

jobs:
  build-and-deploy:
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@v4

      # 1) kubectl ì„¤ì¹˜ (PATHëŠ” ë‹¤ìŒ ìŠ¤í…ë¶€í„° ë°˜ì˜)
      - name: Install kubectl (no-sudo)
        run: |
          set -e
          ver=$(curl -Ls https://dl.k8s.io/release/stable.txt)
          curl -Ls -o "$RUNNER_TEMP/kubectl" "https://dl.k8s.io/release/${ver}/bin/linux/amd64/kubectl"
          chmod +x "$RUNNER_TEMP/kubectl"
          echo "$RUNNER_TEMP" >> "$GITHUB_PATH"

      # 2) kubeconfig ì£¼ì… + ê¸°ë³¸ í™•ì¸
      - name: Configure kube access & verify
        run: |
          set -e
          echo "${{ secrets.KUBECONFIG }}" > "$RUNNER_TEMP/kubeconfig"
          echo "KUBECONFIG=$RUNNER_TEMP/kubeconfig" >> "$GITHUB_ENV"
          kubectl version --client
          kubectl get ns || true

      # 3) ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì—†ìœ¼ë©´ ìƒì„±
      - name: Ensure namespace exists
        run: |
          kubectl get ns "$NAMESPACE" || kubectl create ns "$NAMESPACE"

      # 4) GHCR í‘¸ì‹œìš© ë„ì»¤ ì‹œí¬ë¦¿ (Kanikoê°€ /kaniko/.docker/config.json ì‚¬ìš©)
      - name: Ensure GHCR docker secret
        run: |
          kubectl -n "$NAMESPACE" create secret docker-registry ghcr-creds \
            --docker-server=ghcr.io \
            --docker-username="${{ github.actor }}" \
            --docker-password="${{ secrets.GITHUB_TOKEN }}" \
            --docker-email="ci@example.local" \
            --dry-run=client -o yaml | kubectl apply -f -

      # 5) ì´ì „ Kaniko Job ì •ë¦¬
      - name: Cleanup previous Kaniko Job
        run: |
          kubectl -n "$NAMESPACE" delete job kaniko-build --ignore-not-found=true

      # 6) Kaniko Job ì‹¤í–‰ (ì™¸ë¶€ egress ì—†ì–´ë„ ë™ì‘: runnerê°€ ì†ŒìŠ¤ ë„£ì„ ë•Œê¹Œì§€ ëŒ€ê¸°)
      - name: Launch Kaniko Job
        run: |
          cat <<EOF | kubectl -n "$NAMESPACE" apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: kaniko-build
          spec:
            backoffLimit: 0
            template:
              spec:
                restartPolicy: Never
                initContainers:
                  - name: fetch-src
                    image: alpine:3.20
                    command: ["/bin/sh","-lc"]
                    args:
                      - |
                        set -e
                        mkdir -p /workspace
                        echo ">> waiting for /workspace/.ready ..."
                        while [ ! -f /workspace/.ready ]; do sleep 1; done
                        echo ">> ready, init exit"
                    volumeMounts:
                      - name: workspace
                        mountPath: /workspace
                containers:
                  - name: kaniko
                    image: gcr.io/kaniko-project/executor:latest
                    args:
                      - --context=/workspace
                      - --dockerfile=/workspace/Dockerfile
                      - --destination=${IMAGE}
                      # ìºì‹œ ì›í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                      # - --cache=true
                      # - --cache-repo=${IMAGE_REPO}-cache
                    volumeMounts:
                      - name: docker-config
                        mountPath: /kaniko/.docker/
                      - name: workspace
                        mountPath: /workspace
                volumes:
                  - name: docker-config
                    secret:
                      secretName: ghcr-creds
                      items:
                        - key: .dockerconfigjson
                          path: config.json
                  - name: workspace
                    emptyDir: {}
          EOF

      # 7) ì†ŒìŠ¤ ì¡°ê° ì—…ë¡œë“œ(1MB chunk, ì¬ì‹œë„) â†’ ì¬ì¡°ë¦½ â†’ .ready í„°ì¹˜
      - name: Upload source in chunks & release init
        run: |
          set -e
          # íŒŒë“œê°€ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
          for i in {1..60}; do
            POD=$(kubectl -n "$NAMESPACE" get pods -l job-name=kaniko-build -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
            [ -n "$POD" ] && break
            sleep 2
          done
          echo "POD=$POD"

          # ì›ê²© ì—…ë¡œë“œ ë””ë ‰í„°ë¦¬ ì¤€ë¹„
          kubectl -n "$NAMESPACE" exec "$POD" -c fetch-src -- sh -lc 'rm -rf /workspace/upload && mkdir -p /workspace/upload'

          # ì†ŒìŠ¤ tar ìƒì„± í›„ 1MB ì¡°ê°ìœ¼ë¡œ ë¶„í• 
          TAR="$RUNNER_TEMP/src.tgz"
          tar -C "$GITHUB_WORKSPACE" -czf "$TAR" .
          split -b 1m -d -a 4 "$TAR" "$RUNNER_TEMP/part."

          # ê° ì¡°ê° ì—…ë¡œë“œ (ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„)
          for f in $RUNNER_TEMP/part.*; do
            bn=$(basename "$f")
            ok=0
            for i in {1..6}; do
              if cat "$f" | kubectl -n "$NAMESPACE" exec -i "$POD" -c fetch-src -- sh -lc "cat > /workspace/upload/$bn"; then
                ok=1; break
              fi
              echo "chunk $bn failed; retry $i/6"; sleep 2
            done
            [ "$ok" = "1" ] || (echo "giving up chunk $bn"; exit 1)
          done

          # ë¦¬ëª¨íŠ¸ì—ì„œ ì¬ì¡°ë¦½ + í•´ì œ + .ready ìƒì„±
          kubectl -n "$NAMESPACE" exec "$POD" -c fetch-src -- sh -lc '
            set -e
            cat /workspace/upload/part.* > /workspace/src.tgz
            rm -rf /workspace/upload
            tar -xzf /workspace/src.tgz -C /workspace
            rm /workspace/src.tgz
            touch /workspace/.ready
            ls -lah /workspace | head -n 50
          '

      # 8) Kaniko ì™„ë£Œ ëŒ€ê¸° (ì‹¤íŒ¨ ì‹œ ì§„ë‹¨ ë¡œê·¸ ì¶œë ¥)
      - name: Wait for Kaniko build to finish (with diagnostics)
        run: |
          set -e
          if ! kubectl -n "$NAMESPACE" wait --for=condition=Complete job/kaniko-build --timeout=20m; then
            echo "== JOB FAILED: diagnostics =="
            kubectl -n "$NAMESPACE" get job/kaniko-build -o wide || true
            kubectl -n "$NAMESPACE" describe job/kaniko-build || true
            kubectl -n "$NAMESPACE" get pods -l job-name=kaniko-build -o wide || true
            POD=$(kubectl -n "$NAMESPACE" get pods -l job-name=kaniko-build -o jsonpath='{.items[0].metadata.name}')
            kubectl -n "$NAMESPACE" describe pod "$POD" || true
            echo "== INIT(fetch-src) TAIL =="
            kubectl -n "$NAMESPACE" logs "$POD" -c fetch-src --tail=200 || true
            echo "== KANIKO TAIL =="
            kubectl -n "$NAMESPACE" logs "$POD" -c kaniko --tail=200 || true
            exit 1
          fi

            # 8.5) (ì„ íƒ) í•™ìŠµ Job ì‹¤í–‰ - MLflowì— ê¸°ë¡ ë‚¨ê¹€
      - name: Run training Job (MLflow)
        run: |
          cat <<'EOF' > /tmp/iris-train.yaml
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: iris-train
            labels: { app: iris-train }
          spec:
            ttlSecondsAfterFinished: 600
            backoffLimit: 0
            template:
              spec:
                restartPolicy: Never
                # GHCRê°€ privateì´ë©´ pull ìê²© í•„ìš” (ì´ë¯¸ ìœ„ì—ì„œ ghcr-creds ìƒì„±)
                imagePullSecrets:
                  - name: ghcr-creds
                containers:
                  - name: train
                    image: ${IMAGE}
                    imagePullPolicy: IfNotPresent
                    env:
                      # âœ… MLflow ì„œë²„ ì£¼ì†Œ/ì‹¤í—˜ ì´ë¦„
                      - name: MLFLOW_TRACKING_URI
                        value: http://mlflow.mlops.svc.cluster.local:5000
                      - name: MLFLOW_EXPERIMENT_NAME
                        value: iris-rf
                      # í ½í¿¡ (ì„ íƒ) ì•„í‹°íŒ©íŠ¸ë¥¼ MinIO(S3)ì— ì €ì¥í•˜ë ¤ë©´ ì„œë²„/í´ë¼ ëª¨ë‘ ë™ì¼ ì„¤ì •
                      - name: MLFLOW_S3_ENDPOINT_URL
                        value: http://minio.mlops.svc.cluster.local:9000
                      - name: AWS_ACCESS_KEY_ID
                        valueFrom: { secretKeyRef: { name: minio-creds, key: accessKey } }
                      - name: AWS_SECRET_ACCESS_KEY
                        valueFrom: { secretKeyRef: { name: minio-creds, key: secretKey } }
                    resources:
                      requests: { cpu: "500m", memory: "1Gi" }
                      limits:   { cpu: "2",    memory: "4Gi" }
          EOF

          # ì‹¤í–‰
          kubectl -n "$NAMESPACE" delete job iris-train --ignore-not-found
          kubectl -n "$NAMESPACE" apply -f /tmp/iris-train.yaml

      # 8.6) (ì˜µì…˜) í•™ìŠµ ë¡œê·¸ ë”°ë¼ë¶™ê¸° â€” ì‹¤íŒ¨í•´ë„ ì›Œí¬í”Œë¡œëŠ” ê³„ì†
      - name: Tail training logs (best-effort)
        continue-on-error: true
        run: |
          kubectl -n "$NAMESPACE" logs -f job/iris-train -c train --since=1s || true


      # 9) ë°°í¬ ì ìš© ë° ì´ë¯¸ì§€ íƒœê·¸ ê°±ì‹ 
      - name: Set image and rollout
        run: |
          set -e
          kubectl -n "$NAMESPACE" apply -f k8s/deploy.yaml
          kubectl -n "$NAMESPACE" set image deploy/${APP_NAME} ${APP_NAME}=${IMAGE} --record
          kubectl -n "$NAMESPACE" rollout status deploy/${APP_NAME} --timeout=5m

