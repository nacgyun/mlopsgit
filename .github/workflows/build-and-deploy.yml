name: Train with MLflow (Runtime image on demand, content-hash tagging)

on:
  workflow_dispatch:
  push:
    branches: [ master ]

permissions:
  contents: read
  packages: read
  actions: write  # 'workflows' 키는 없음

jobs:
  train-build-run:
    runs-on: [self-hosted, k8s]

    env:
      NAMESPACE: mlops
      JOB_BASENAME: train-job
      IMAGE_RUNTIME: ghcr.io/nacgyun/mlopsgit:runtime
      GITHUB_OWNER: nacgyun
      GITHUB_REPO: mlopsgit
      PROMOTE_THRESHOLD: "0.97"
      MINIO_ENDPOINT: http://minio.mlops.svc.cluster.local:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      DATA_BUCKET: data
      DATA_KEY: telco/WA_Fn-UseC_-Telco-Customer-Churn.csv
      MODEL_BUCKET: models

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # 필수 시크릿 체크
      - name: Assert required secrets (no values logged)
        shell: bash
        env:
          S_GHCR: ${{ secrets.GHCR_PAT }}
          S_PROMOTE: ${{ secrets.PROMOTE_PAT }}
        run: |
          set -euo pipefail
          for k in S_GHCR S_PROMOTE; do
            v="${!k:-}"
            if [ -z "$v" ]; then
              echo "::error title=Missing secret::$k is EMPTY"; exit 1
            else
              echo "$k=OK(len=$(echo -n "$v" | wc -c))"
            fi
          done

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install kubectl if missing
        run: |
          set -e
          if ! command -v kubectl >/dev/null 2>&1; then
            ARCH=$(uname -m); case "$ARCH" in x86_64) ARCH=amd64;; aarch64) ARCH=arm64;; esac
            curl -sSL -o kubectl "https://dl.k8s.io/release/$(curl -sSL https://dl.k8s.io/release/stable.txt)/bin/linux/${ARCH}/kubectl"
            chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          fi
          kubectl version --client=true

      - name: Compute vars
        id: vars
        run: |
          set -euo pipefail
          echo "GIT_SHA=${GITHUB_SHA}" >> $GITHUB_ENV
          echo "SHORT_SHA=$(echo "${GITHUB_SHA}" | cut -c1-12)" >> $GITHUB_ENV
          echo "RUN_ID=${GITHUB_RUN_ID}" >> $GITHUB_ENV
          echo "BUILD_ID=${GITHUB_RUN_NUMBER}-${GITHUB_RUN_ATTEMPT}" >> $GITHUB_ENV
          echo "OWNER_LC=$(echo "${GITHUB_OWNER:-$GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
          echo "REPO_LC=$(echo "${GITHUB_REPO:-${GITHUB_REPOSITORY#*/}}" | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
          echo "NS_LC=$(echo "${NAMESPACE}" | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
          echo "JOB_NAME=$(echo "${JOB_BASENAME}-${GITHUB_RUN_ID}" | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV

      - name: Detect changes for runtime image (ref only)
        id: changes
        uses: dorny/paths-filter@v3
        with:
          token: ${{ github.token }}
          filters: |
            runtime_image:
              - 'ml/requirements.txt'
              - 'Dockerfile.runtime'
              - 'Dockerfile'

      - name: Check runtime tag by BUILD_KEY
        id: tagcheck
        env:
          GHCR_PAT: ${{ secrets.GHCR_PAT }}
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update -y >/dev/null 2>&1 || true
            sudo apt-get install -y jq >/dev/null 2>&1 || true
          fi

          OWNER="${GITHUB_OWNER:-nacgyun}"
          REPO="${GITHUB_REPO:-mlopsgit}"
          TOKEN="${GHCR_PAT:-$GITHUB_TOKEN}"

          if [ -f Dockerfile.runtime ] && [ -f ml/requirements.txt ]; then
            BUILD_KEY="$( (cat Dockerfile.runtime; echo; cat ml/requirements.txt) | sha256sum | awk '{print $1}')"
          elif [ -f Dockerfile.runtime ]; then
            BUILD_KEY="$(sha256sum Dockerfile.runtime | awk '{print $1}')"
          else
            BUILD_KEY=""
          fi
          echo "build_key=${BUILD_KEY}" >> "$GITHUB_OUTPUT"

          tag_exists() {
            local tag="runtime-$1"
            curl -fsSL -H "Authorization: Bearer ${TOKEN}" \
              "https://ghcr.io/v2/${OWNER}/${REPO}/manifests/${tag}" \
              -H "Accept: application/vnd.docker.distribution.manifest.v2+json" >/dev/null
          }

          NEED_BUILD=false
          if [ -z "${BUILD_KEY}" ]; then
            NEED_BUILD=true
          elif tag_exists "${BUILD_KEY}"; then
            NEED_BUILD=false
          else
            NEED_BUILD=true
          fi
          echo "need_build=${NEED_BUILD}" >> "$GITHUB_OUTPUT"

      # GHCR 인증 config.json을 Kaniko가 읽도록 보장
      - name: Ensure GHCR dockerconfig secret (ghcr-push)
        shell: bash
        env:
          GHCR_PAT: ${{ secrets.GHCR_PAT }}
          GH_USER: ${{ env.GITHUB_OWNER }}
        run: |
          set -euo pipefail
          : "${NAMESPACE:?NAMESPACE missing}"
          kubectl -n "${NAMESPACE}" delete secret ghcr-push --ignore-not-found || true
          AUTH_B64="$(printf '%s:%s' "${GH_USER}" "${GHCR_PAT}" | base64 -w0 2>/dev/null || printf '%s' "${GH_USER}:${GHCR_PAT}" | base64)"
          cat > /tmp/config.json <<EOF
          {
            "auths": {
              "ghcr.io": { "auth": "${AUTH_B64}" }
            }
          }
          EOF
          kubectl -n "${NAMESPACE}" create secret generic ghcr-push --from-file=config.json=/tmp/config.json
          kubectl -n "${NAMESPACE}" get secret ghcr-push -o yaml | head -n 20

      # === 태그 없으면 Kaniko 빌드 실행 ===
      - name: Kaniko runtime-build (render → apply)
        if: steps.tagcheck.outputs.need_build == 'true'
        env:
          KANIKO_JOB_NAME: kaniko-build-${{ github.run_id }}
        run: |
          set -euxo pipefail
          : "${KANIKO_JOB_NAME:?}"; : "${IMAGE_RUNTIME:?}"; : "${GIT_SHA:?}"; : "${NS_LC:?}"

          sed -e "s|__KANIKO_JOB_NAME__|${KANIKO_JOB_NAME}|g" \
              -e "s|__NAMESPACE__|${NS_LC}|g" \
              -e "s|__GITHUB_OWNER__|${OWNER_LC}|g" \
              -e "s|__GITHUB_REPO__|${REPO_LC}|g" \
              -e "s|__GIT_SHA__|${GIT_SHA}|g" \
              -e "s|__BUILD_KEY__|${{ steps.tagcheck.outputs.build_key }}|g" \
            k8s/kaniko-job.yaml.tmpl > /tmp/kaniko-job.yaml

          if grep -Eq '__(KANIKO_JOB_NAME|NAMESPACE|GITHUB_OWNER|GITHUB_REPO|GIT_SHA|BUILD_KEY)__' /tmp/kaniko-job.yaml; then
            echo "::error ::placeholder left in kaniko-job.yaml"; sed -n '1,200p' /tmp/kaniko-job.yaml; exit 1
          fi

          echo "----- rendered kaniko-job -----"; sed -n '1,200p' /tmp/kaniko-job.yaml

          kubectl -n "${NS_LC}" delete job "${KANIKO_JOB_NAME}" --ignore-not-found --wait=true || true
          kubectl -n "${NS_LC}" apply -f /tmp/kaniko-job.yaml
          kubectl -n "${NS_LC}" wait --for=condition=complete --timeout=45m "job/${KANIKO_JOB_NAME}"

          # 빌드 로그 요약
          kubectl -n "${NS_LC}" logs -l job-name="${KANIKO_JOB_NAME}" -c kaniko --tail=-1 | egrep -i 'Pushing|Pushed|digest' || true

      # Kaniko 실패 시 자동 덤프
      - name: Dump Kaniko job logs on failure
        if: failure()
        shell: bash
        env:
          NS: ${{ env.NS_LC }}
          KANIKO_JOB_NAME: kaniko-build-${{ github.run_id }}
        run: |
          set -euo pipefail
          echo "---- describe job ----"; kubectl -n "${NS}" describe job "${KANIKO_JOB_NAME}" || true
          echo "---- pods ----"; kubectl -n "${NS}" get pods -l job-name="${KANIKO_JOB_NAME}" -o wide || true
          for p in $(kubectl -n "${NS}" get pods -l job-name="${KANIKO_JOB_NAME}" -o name 2>/dev/null); do
            echo "---- logs $p (kaniko) ----"; kubectl -n "${NS}" logs "$p" -c kaniko --tail=500 || true
            echo "---- describe $p ----"; kubectl -n "${NS}" describe "$p" || true
          done

      - name: Compute IMAGE tag (prefer BUILD_KEY; fallback SHA)
        run: |
          set -e
          RKEY="${{ steps.tagcheck.outputs.build_key }}"
          if [ -z "$RKEY" ]; then RKEY="${GIT_SHA}"; fi
          echo "IMAGE_RUNTIME_KEY=${RKEY}" >> $GITHUB_ENV
          echo "IMAGE_RUNTIME_SHA=${IMAGE_RUNTIME}-${RKEY}" >> $GITHUB_ENV

      - name: Render Train Job manifest
        run: |
          set -euo pipefail
          sed -e "s|__JOB_NAME__|${JOB_NAME}|g" \
              -e "s|__NAMESPACE__|${NS_LC}|g" \
              -e "s|__RUN_ID__|${RUN_ID}|g" \
              -e "s|__BUILD_ID__|${BUILD_ID}|g" \
              -e "s|__GIT_SHA__|${GIT_SHA}|g" \
              -e "s|__IMAGE__|${IMAGE_RUNTIME_SHA}|g" \
              -e "s|__GITHUB_OWNER__|${OWNER_LC}|g" \
              -e "s|__GITHUB_REPO__|${REPO_LC}|g" \
              -e "s|__GHCR_IMAGE__|${GHCR_IMAGE:-}|g" \
              -e "s|__GHCR_TAG__|${GHCR_TAG:-}|g" \
              -e "s|__GHCR_DIGEST__|${GHCR_DIGEST:-}|g" \
              -e "s|__GHCR_IMAGE_REF__|${GHCR_IMAGE_REF:-}|g" \
            k8s/train-job.yaml.tmpl > /tmp/train-job.yaml

          if grep -qE "__[A-Z0-9_]+__" /tmp/train-job.yaml; then
            echo "::error ::placeholder left in train-job.yaml"
            grep -nE "__[A-Z0-9_]+__" /tmp/train-job.yaml || true
            exit 1
          fi

          echo "----- rendered train-job -----"; sed -n '1,240p' /tmp/train-job.yaml

      - name: Apply Train Job
        run: |
          set -euo pipefail
          kubectl -n "${NS_LC}" delete job "${JOB_NAME}" --ignore-not-found --wait=true || true
          kubectl -n "${NS_LC}" apply -f /tmp/train-job.yaml
          kubectl -n "${NS_LC}" get job "${JOB_NAME}"

      - name: Wait for Train Pod to appear
        run: |
          set -euo pipefail
          echo "[wait] waiting pod of job=${JOB_NAME}"
          POD_NAME=""
          for i in {1..60}; do
            POD_NAME="$(kubectl -n "${NS_LC}" get pods -l job-name="${JOB_NAME}" --no-headers 2>/dev/null | awk 'NR==1{print $1}')"
            if [ -n "${POD_NAME}" ]; then
              echo "POD_NAME=${POD_NAME}" >> "$GITHUB_ENV"
              echo "[wait] pod=${POD_NAME}"
              break
            fi
            sleep 5
          done
          if [ -z "${POD_NAME}" ]; then
            echo "::error ::Pod not appeared"
            kubectl -n "${NS_LC}" get pods -l job-name="${JOB_NAME}" -o wide || true
            kubectl -n "${NS_LC}" describe job "${JOB_NAME}" || true
            exit 1
          fi

      - name: Wait Train → logs
        run: |
          set -euo pipefail
          kubectl -n "${NS_LC}" wait --for=condition=complete --timeout=60m "job/${JOB_NAME}"
          kubectl -n "${NS_LC}" logs -l job-name="${JOB_NAME}" -c trainer --tail=300 || true
          kubectl -n "${NS_LC}" logs -l job-name="${JOB_NAME}" -c fetch-src --tail=100 || true

      - name: Parse accuracy from trainer logs
        id: read_acc
        run: |
          set -euo pipefail
          LINE="$(kubectl -n "${NS_LC}" logs -l job-name="${JOB_NAME}" --all-containers=true --tail=-1 | tac | grep -m1 '^\[PROMOTE\] accuracy=' || true)"
          if [ -z "${LINE}" ]; then
            echo "acc=NA" >>"$GITHUB_OUTPUT"; echo "promote=no" >>"$GITHUB_OUTPUT"; exit 0; fi
          ACC="$(echo "${LINE}" | sed -n 's/.*accuracy=\([0-9.]\+\).*/\1/p')"
          if [ -z "${ACC}" ]; then
            echo "acc=NA" >>"$GITHUB_OUTPUT"; echo "promote=no" >>"$GITHUB_OUTPUT"; exit 0; fi
          DECIDE="$(awk -v a="${ACC}" -v t="${PROMOTE_THRESHOLD}" 'BEGIN{print (a+0>=t+0)?"yes":"no"}')"
          echo "acc=${ACC}" >>"$GITHUB_OUTPUT"; echo "promote=${DECIDE}" >>"$GITHUB_OUTPUT"

      - name: Extract MLflow RUN_ID from trainer logs
        id: runid
        if: steps.read_acc.outputs.promote == 'yes'
        run: |
          set -euo pipefail
          LINE="$(kubectl -n "${NS_LC}" logs -l job-name="${JOB_NAME}" --all-containers=true --tail=-1 | tac | grep -m1 '^\[mlflow\] run_id=' || true)"
          RUN_ID_VAL="$(echo "${LINE}" | sed -n 's/.*run_id=\([A-Za-z0-9]\{8,64\}\).*/\1/p')"
          if [ -z "${RUN_ID_VAL}" ]; then echo "::error ::RUN_ID not found"; exit 1; fi
          echo "run_id=${RUN_ID_VAL}" >> "$GITHUB_OUTPUT"

      - name: Dispatch promote-selected-model (auto)
        if: steps.read_acc.outputs.promote == 'yes'
        env:
          OWNER: ${{ env.OWNER_LC }}
          REPO:  ${{ env.REPO_LC }}
          BRANCH: master
          PROMOTE_PAT: ${{ secrets.PROMOTE_PAT }}
          RUN_ID_PICKED: ${{ steps.runid.outputs.run_id }}
        run: |
          set -euo pipefail
          if ! command -v jq >/dev/null 2>&1; then sudo apt-get update -y >/dev/null 2>&1 || true; sudo apt-get install -y jq >/dev/null 2>&1 || true; fi
          if [ -z "${PROMOTE_PAT:-}" ]; then echo "::error ::PROMOTE_PAT missing"; exit 1; fi
          BODY=$(jq -nc --arg ref "$BRANCH" --arg model "light-logreg" --arg run "$RUN_ID_PICKED" --arg stage "Production" '{ref:$ref, inputs:{model_name:$model, run_id:$run, serve_stage:$stage}}')
          curl -fsSL -X POST \
            -H "Authorization: Bearer ${PROMOTE_PAT}" \
            -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/${OWNER}/${REPO}/actions/workflows/promote-selected-model.yml/dispatches" \
            -d "$BODY"
          echo "[ok] dispatched"

      - name: On failure dump diagnostics
        if: failure()
        run: |
          set -euo pipefail
          echo "---- describe job ----"; kubectl -n "${NS_LC}" describe job "${JOB_NAME}" || true
          echo "---- list pods ----"; kubectl -n "${NS_LC}" get pods -l job-name="${JOB_NAME}" -o wide || true
          echo "---- describe pods ----"
          for p in $(kubectl -n "${NS_LC}" get pods -l job-name="${JOB_NAME}" -o name 2>/dev/null); do kubectl -n "${NS_LC}" describe "$p" || true; done
          echo "---- init logs ----"; kubectl -n "${NS_LC}" logs -l job-name="${JOB_NAME}" -c fetch-src --tail=200 || true
          echo "---- trainer logs ----"; kubectl -n "${NS_LC}" logs -l job-name="${JOB_NAME}" -c trainer --tail=500 || true

