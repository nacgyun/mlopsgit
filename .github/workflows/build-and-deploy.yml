name: Train with MLflow (Runtime image on demand, content-hash tagging)

on:
  workflow_dispatch:
  push:
    branches: [ master ]

jobs:
  train-build-run:
    runs-on: [self-hosted, k8s]

    env:
      NAMESPACE: mlops
      TRAIN_JOB_NAME: train-job
      IMAGE_RUNTIME: ghcr.io/nacgyun/mlopsgit:runtime
      GITHUB_OWNER: nacgyun
      GITHUB_REPO: mlopsgit

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install kubectl if missing
        run: |
          set -e
          if ! command -v kubectl >/dev/null 2>&1; then
            ARCH=$(uname -m); case "$ARCH" in x86_64) ARCH=amd64;; aarch64) ARCH=arm64;; esac
            curl -sSL -o kubectl "https://dl.k8s.io/release/$(curl -sSL https://dl.k8s.io/release/stable.txt)/bin/linux/${ARCH}/kubectl"
            chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          fi
          kubectl version --client=true

      - name: Get vars
        id: vars
        run: |
          set -e
          echo "GIT_SHA=${GITHUB_SHA}" >> $GITHUB_ENV
          echo "SHORT_SHA=$(echo "${GITHUB_SHA}" | cut -c1-12)" >> $GITHUB_ENV

      - name: Detect changes for runtime image (ref only)
        id: changes
        uses: dorny/paths-filter@v3
        with:
          token: ${{ github.token }}
          filters: |
            runtime_image:
              - 'ml/requirements.txt'
              - 'Dockerfile.runtime'
              - 'Dockerfile'

      - name: Check runtime tag by BUILD_KEY
        id: tagcheck
        env:
          GHCR_PAT: ${{ secrets.GHCR_PAT }}      # read:packages 이상 권장
          GITHUB_TOKEN: ${{ github.token }}      # fallback
        run: |
          set -euo pipefail
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update -y >/dev/null 2>&1 || true
            sudo apt-get install -y jq >/dev/null 2>&1 || true
          fi

          OWNER="${GITHUB_OWNER:-nacgyun}"
          REPO="${GITHUB_REPO:-mlopsgit}"
          TOKEN="${GHCR_PAT:-$GITHUB_TOKEN}"

          # BUILD_KEY: Dockerfile.runtime + ml/requirements.txt 해시
          if [ -f Dockerfile.runtime ] && [ -f ml/requirements.txt ]; then
            BUILD_KEY="$( (cat Dockerfile.runtime; echo; cat ml/requirements.txt) | sha256sum | awk '{print $1}')"
          elif [ -f Dockerfile.runtime ]; then
            BUILD_KEY="$(sha256sum Dockerfile.runtime | awk '{print $1}')"
          else
            BUILD_KEY=""
          fi
          echo "build_key=${BUILD_KEY}" >> "$GITHUB_OUTPUT"

          has_build_tag() {
            curl -fsSL -H "Authorization: Bearer ${TOKEN}" \
              "https://ghcr.io/v2/${OWNER}/${REPO}/tags/list" \
              | jq -e --arg t "runtime-${BUILD_KEY}" '.tags[]? | select(.==$t)' >/dev/null
          }

          NEED_BUILD=false
          if [ -z "${BUILD_KEY}" ]; then
            echo "BUILD_KEY empty → build required"
            NEED_BUILD=true
          elif has_build_tag; then
            echo "Tag runtime-${BUILD_KEY} exists → skip build"
            NEED_BUILD=false
          else
            echo "No tag runtime-${BUILD_KEY} → build required"
            NEED_BUILD=true
          fi

          echo "need_build=${NEED_BUILD}" >> "$GITHUB_OUTPUT"

      - name: Kaniko runtime-build (render → apply)
        if: steps.tagcheck.outputs.need_build == 'true' && steps.changes.outputs.runtime_image == 'true'
        env:
          KANIKO_JOB_NAME: kaniko-build-${{ github.run_id }}
        run: |
          set -euxo pipefail
          : "${KANIKO_JOB_NAME:?}"; : "${IMAGE_RUNTIME:?}"; : "${GIT_SHA:?}"; : "${NAMESPACE:?}"

          sed -e "s|__KANIKO_JOB_NAME__|${KANIKO_JOB_NAME}|g" \
              -e "s|__NAMESPACE__|${NAMESPACE}|g" \
              -e "s|__GITHUB_OWNER__|${GITHUB_OWNER}|g" \
              -e "s|__GITHUB_REPO__|${GITHUB_REPO}|g" \
              -e "s|__GIT_SHA__|${GIT_SHA}|g" \
              -e "s|__BUILD_KEY__|${{ steps.tagcheck.outputs.build_key }}|g" \
            k8s/kaniko-job.yaml.tmpl > /tmp/kaniko-job.yaml

          # 렌더 가드
          if grep -Eq '__(KANIKO_JOB_NAME|NAMESPACE|GITHUB_OWNER|GITHUB_REPO|GIT_SHA|BUILD_KEY)__' /tmp/kaniko-job.yaml; then
            echo "❌ placeholder left in kaniko-job.yaml"; sed -n '1,180p' /tmp/kaniko-job.yaml; exit 1
          fi

          kubectl -n "${NAMESPACE}" create --dry-run=client -f /tmp/kaniko-job.yaml -o yaml >/dev/null
          echo "----- rendered kaniko-job -----"; sed -n '1,200p' /tmp/kaniko-job.yaml

          kubectl -n "${NAMESPACE}" delete job "${KANIKO_JOB_NAME}" --ignore-not-found || true
          kubectl -n "${NAMESPACE}" apply -f /tmp/kaniko-job.yaml
          kubectl -n "${NAMESPACE}" wait --for=condition=complete --timeout=30m "job/${KANIKO_JOB_NAME}"

          POD=$(kubectl -n "${NAMESPACE}" get pod -l job-name="${KANIKO_JOB_NAME}" -o jsonpath='{.items[0].metadata.name}')
          kubectl -n "${NAMESPACE}" logs "$POD" -c kaniko --tail=-1 | egrep -i 'Pushing|Pushed|digest' || true

      - name: Compute IMAGE tag (prefer BUILD_KEY; fallback SHA)
        run: |
          set -e
          RKEY="${{ steps.tagcheck.outputs.build_key }}"
          if [ -z "$RKEY" ]; then RKEY="${GIT_SHA}"; fi
          echo "IMAGE_RUNTIME_KEY=${RKEY}" >> $GITHUB_ENV
          echo "IMAGE_RUNTIME_SHA=${IMAGE_RUNTIME}-${RKEY}" >> $GITHUB_ENV

      - name: Render Train Job manifest
        run: |
          set -e
          sed -e "s|__IMAGE__|${IMAGE_RUNTIME_SHA}|g" \
              -e "s|__GIT_SHA__|${GIT_SHA}|g" \
              -e "s|__NAMESPACE__|${NAMESPACE}|g" \
              -e "s|__GITHUB_OWNER__|${GITHUB_OWNER}|g" \
              -e "s|__GITHUB_REPO__|${GITHUB_REPO}|g" \
            k8s/train-job.yaml.tmpl > /tmp/train-job.yaml

          # 렌더 가드
          if grep -Eq '__(NAMESPACE|IMAGE|GIT_SHA|GITHUB_OWNER|GITHUB_REPO)__' /tmp/train-job.yaml; then
            echo "❌ placeholder left in train-job.yaml"; sed -n '1,200p' /tmp/train-job.yaml; exit 1
          fi

          echo "----- rendered train-job -----"; sed -n '1,220p' /tmp/train-job.yaml

      - name: Apply Train Job
        run: |
          set -e
          kubectl -n "${NAMESPACE}" delete job "${TRAIN_JOB_NAME}" --ignore-not-found || true
          kubectl -n "${NAMESPACE}" apply -f /tmp/train-job.yaml
          kubectl -n "${NAMESPACE}" get pods -l job-name="${TRAIN_JOB_NAME}" -o wide || true

      - name: Wait Train → logs
        run: |
          set -e
          kubectl -n "${NAMESPACE}" wait --for=condition=complete --timeout=60m job/${TRAIN_JOB_NAME}
          POD=$(kubectl -n "${NAMESPACE}" get pod -l job-name="${TRAIN_JOB_NAME}" -o jsonpath='{.items[0].metadata.name}')
          kubectl -n "${NAMESPACE}" logs "$POD" --tail=300 || true

